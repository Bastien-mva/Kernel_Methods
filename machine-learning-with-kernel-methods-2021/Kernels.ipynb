{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from itertools import product, permutations\n",
    "\n",
    "import numba\n",
    "from numba import njit,vectorize, jit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@njit\n",
    "sig2 = 1\n",
    "@njit\n",
    "def GaussianKernel(x,y):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2/(2*sig2))\n",
    "\n",
    "@njit\n",
    "def PolynomialKernel(x,y): \n",
    "    return (x.dot(y)+c)**d\n",
    "@njit\n",
    "def LinearKernel(x,y): \n",
    "    return x.dot(y)\n",
    "\n",
    "@njit\n",
    "def Laplace_kernel(x, y, gamma=1):\n",
    "    return 0.5 * np.exp(-gamma * np.linalg.norm(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importation_mat_100(): \n",
    "    '''\n",
    "    import all the (float) data and puts them in a list X_train_100. X_train_100[0] will be X_train_mat_100 and so on. \n",
    "    '''\n",
    "    X_train_100 = []\n",
    "    X_test_100 = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtrain = np.squeeze(xtrain.to_numpy())\n",
    "        X_train_100.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtest = np.squeeze(xtest.to_numpy())\n",
    "        X_test_100.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train_100,X_test_100,Y_train\n",
    "\n",
    "def importation(): \n",
    "    '''\n",
    "    import all the (string) data and puts them in a list X_train. X_train[0] will be X_train_0 and so on. \n",
    "    '''\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtrain = xtrain.iloc[1:,1].to_numpy()\n",
    "        X_train.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtest = xtest.iloc[1:,1].to_numpy()\n",
    "        X_test.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train,X_test,Y_train\n",
    "\n",
    "X_train,X_test,Y_train = importation()\n",
    "X_train_100, X_test_100,Y_train = importation_mat_100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we do this so that we compute the Dictionnary of phi_spectrum easier. \n",
    "'''\n",
    "\n",
    "X_traintest = []\n",
    "for i in range(3): \n",
    "    X_traintest.append(np.concatenate((X_train[i],X_test[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Kernel_train(X, Kernel): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = np.squeeze(X[i])\n",
    "        for j in range(i,length): \n",
    "            x_j = np.squeeze(X[j])\n",
    "            value = Kernel(x_i,x_j)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "#@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel):\n",
    "    '''\n",
    "    takes the training data input Xtrain and the test data Xtest and computes the Kernel_test. \n",
    "    \n",
    "    The length of the resulting Kernel_test will be (nb_traing_samples, nb_testing_samples) \n",
    "    '''\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in range(length_train):\n",
    "        x_i = np.squeeze(Xtrain[i])\n",
    "        for j in range(length_test): \n",
    "            x_j = np.squeeze(Xtest[j])\n",
    "            bimat_K[i,j] = Kernel(x_i,x_j)\n",
    "    return bimat_K\n",
    "\n",
    "def standardize(K): \n",
    "    '''\n",
    "    standardize the given matrix K\n",
    "    '''\n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phi for Spectrum kernel\n",
    "def phi_spectrum(x,k,U):\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi_spec = np.zeros(len(U))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in range(len(x)-k+1):\n",
    "            if x[i:i+k] == u:\n",
    "                phi_spec[j] +=1\n",
    "    return phi_spec\n",
    "\n",
    "#Compute phi for Substring kernel\n",
    "def l(i):\n",
    "    return i[-1] - i[0] + 1\n",
    "\n",
    "def I(k,n):\n",
    "    I = set()\n",
    "    for ele in permutations(range(0,n),k):\n",
    "        I.add(tuple(sorted(list(ele))))\n",
    "    return list(I)\n",
    "\n",
    "def phi_substring(x,k,U,lamb=0.5): #fonctionne (testé avec l'exemple page 392)\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi = np.zeros(len(U))\n",
    "    I_kn = I(k,len(x))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in I_kn:\n",
    "            x_i = \"\".join([x[idx]  for idx in i])\n",
    "            if x_i==u:\n",
    "                phi[j] += lamb**l(i)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def make_dict_phi(X, phi,k):\n",
    "    U = [''.join(letter) for letter in product('ACGT', repeat=k)]\n",
    "    phi_dict = {seq:phi(seq,k,U) for seq in tqdm(X)}\n",
    "    return phi_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [01:35<00:00, 94.49it/s] \n",
      "100%|██████████| 9000/9000 [06:13<00:00, 24.08it/s]\n"
     ]
    }
   ],
   "source": [
    "##CONSTRUCTION DES DICTIONNAIRES. On fait ça pour eviter de les recalculer à chaque fois\n",
    "## just uncomment the ones you want to compute. (and uncomment the next line also). for k=8, takes about 2 hours. \n",
    "#dict_spectrum_traintest = [0]*10\n",
    "\n",
    "#dict_spectrum_traintest[3] = make_dict_phi(X_traintest[0],phi_spectrum,3)\n",
    "#dict_spectrum_traintest[4] = make_dict_phi(X_traintest[0],phi_spectrum,4)\n",
    "\n",
    "#dict_spectrum_traintest[5] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,5)\n",
    "#dict_spectrum_traintest[6] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,6)\n",
    "#dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)\n",
    "#dict_spectrum_traintest[8] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,8)\n",
    "#dict_sub_traintest_2 = make_dict_phi(X_traintest[0], phi_substring,2)\n",
    "\n",
    "#dict_sub_traintest_3 = make_dict_phi(X_traintest[0], phi_substring,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [24:52<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_spectrum(k): \n",
    "    '''\n",
    "    template function to build a K_spectrum_function. \n",
    "    We need to have one function for each kernel functionin order to compute efficiently the Kernel matrix (it makes code fluent). \n",
    "    '''\n",
    "    def K_spectrum(x,y): \n",
    "        value = np.sum([dict_spectrum_traintest[k][str(x)] * dict_spectrum_traintest[k][str(y)]])\n",
    "        return value \n",
    "    return K_spectrum\n",
    "K_spectrum_5 = make_K_spectrum(5)\n",
    "K_spectrum_6 = make_K_spectrum(6)\n",
    "K_spectrum_7 = make_K_spectrum(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 2., 3.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_spectrum_traintest[5][X_train[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_Kernels(Kernel): \n",
    "    '''\n",
    "    gets the first Kernel_train and Kernel_test. This is only for testing one model one the first dataset. \n",
    "    '''\n",
    "    K_train = to_Kernel_train(X_train[0], Kernel)\n",
    "    K_test = to_Kernel_test(X_train[0], X_test[0], Kernel)\n",
    "    return K_train,K_test \n",
    "\n",
    "def get_list_Kernels_100(Kernel) : \n",
    "    '''\n",
    "    get two list ok Kernel. For list_K_train, each element i is the Kernel_train computed with kernel for dataset i. Same for list_K_test\n",
    "    args : \n",
    "        \n",
    "        Kernel : a Kernel function, for example K_sprectum_6\n",
    "        \n",
    "    returns : \n",
    "    \n",
    "            List_Kernel_train : a list where each element i is the Kernel_train computed with Kernel for dataset i.\n",
    "            \n",
    "            List_Kernel_test : a list where each element i is the Kernel_test computed with Kernel for dataset i. \n",
    "            \n",
    "    Only works for Kernel that takes as input floats, for example GaussianKernel\n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train_100[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train_100[i],X_test_100[i], Kernel))\n",
    "    return list_K_train,list_K_test\n",
    "\n",
    "def get_list_Kernels(Kernel) : \n",
    "    '''\n",
    "    \n",
    "    get two list ok Kernel. For list_K_train, each element i is the Kernel_train computed with kernel for dataset i. Same for list_K_test\n",
    "    args : \n",
    "        \n",
    "        Kernel : a Kernel function, for example K_sprectum_6\n",
    "        \n",
    "    returns : \n",
    "    \n",
    "            List_Kernel_train : a list where each element i is the Kernel_train computed with Kernel for dataset i.\n",
    "            \n",
    "            List_Kernel_test : a list where each element i is the Kernel_test computed with Kernel for dataset i. \n",
    "            \n",
    "    Only works for Kernel that takes as input strings, for example K_spectrum. \n",
    "    \n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train[i],X_test[i], Kernel))\n",
    "    return list_K_train,list_K_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:33<00:00, 111.33s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_6, list_K_test_spectrum_6 = get_list_Kernels(K_spectrum_6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:21<00:00, 107.27s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_5, list_K_test_spectrum_5 = get_list_Kernels(K_spectrum_5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [09:34<00:00, 191.66s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_7, list_K_test_spectrum_7 = get_list_Kernels(K_spectrum_7)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(model, Kernel_train, Y_train, parameters): \n",
    "    '''\n",
    "    grid search cv. \n",
    "    put the model you want, for example Estimators.KRR(), the Kernel_train you want to train on, the labels Y_train\n",
    "    and the list of parameters (parameters) you want to try. \n",
    "    \n",
    "    Will print the best parameter with the score associated. \n",
    "    '''\n",
    "    scores = list()\n",
    "    for parameter in parameters : \n",
    "        model.set_parameter(parameter)\n",
    "        scores.append(model.cross_val(Kernel_train,Y_train, 5))\n",
    "        print('We have tested  : ', np.round(parameter, 6))\n",
    "    arg_max = np.argmax(np.array(scores))\n",
    "    print('###########BEST_PARAM = ',np.round(parameters[arg_max], 6), 'with score :', scores[arg_max])\n",
    "    return parameters[arg_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.12it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7105\n",
      "We have tested  :  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.08it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.716\n",
      "We have tested  :  0.006993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  7.06it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7275\n",
      "We have tested  :  0.013886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7295\n",
      "We have tested  :  0.020779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.44it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.736\n",
      "We have tested  :  0.027672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.48it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7355\n",
      "We have tested  :  0.034566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.59it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7344999999999999\n",
      "We have tested  :  0.041459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.739\n",
      "We have tested  :  0.048352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.83it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7384999999999999\n",
      "We have tested  :  0.055245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.48it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7344999999999999\n",
      "We have tested  :  0.062138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.03it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7404999999999999\n",
      "We have tested  :  0.069031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.48it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.731\n",
      "We have tested  :  0.075924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.60it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.74\n",
      "We have tested  :  0.082817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.65it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7295\n",
      "We have tested  :  0.08971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.20it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7415\n",
      "We have tested  :  0.096603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.739\n",
      "We have tested  :  0.103497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 14.37it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.731\n",
      "We have tested  :  0.11039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.29it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.736\n",
      "We have tested  :  0.117283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.89it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7335\n",
      "We have tested  :  0.124176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.34it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7255\n",
      "We have tested  :  0.131069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.01it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7344999999999999\n",
      "We have tested  :  0.137962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.743\n",
      "We have tested  :  0.144855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.729\n",
      "We have tested  :  0.151748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.20it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7344999999999999\n",
      "We have tested  :  0.158641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7304999999999999\n",
      "We have tested  :  0.165534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.87it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7255\n",
      "We have tested  :  0.172428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.96it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7364999999999999\n",
      "We have tested  :  0.179321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.90it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7315\n",
      "We have tested  :  0.186214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.79it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7264999999999999\n",
      "We have tested  :  0.193107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.734\n",
      "We have tested  :  0.2\n",
      "###########BEST_PARAM =  0.144855 with score : 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14485517241379312"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KRR_spectrum_7 = Estimators.KRR(Kernel = K_spectrum_7)\n",
    "parameters = np.linspace(0.0001,0.2,30)\n",
    "#parameters = 10**np.linspace(-10,0,15)\n",
    "grid_search_cv(KRR_spectrum_7, list_K_train_spectrum_7[2], Y_train[2], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_results(model,list_Kernel_train,list_Kernel_test, parameters, name_dossier):\n",
    "    '''\n",
    "    Download the predictions of the 3 datasets, where the model will be trained with the Kernel_train you give and the parameter you give. \n",
    "    \n",
    "    args : \n",
    "            model : a model for example Estimators.KRR()\n",
    "            list_Kernel_train : a list where the element i is the Kernel_train of the i dataset (not any Kernel_train, the one computed with the Kernel of your choice)\n",
    "            list_Kernel_test : a list where the element i is the Kernel_test of the i dataset (not any Kernel_test, the one computed with the Kernel of your choice)\n",
    "            parameters : a list of parameters you want to try. \n",
    "            name_dossier : your predictions will be stocked in the dossier 'predictions_KM'+name_dossier+'.csv'\n",
    "            \n",
    "    returns : None. \n",
    "    \n",
    "    All your predictions will be directly saved. \n",
    "    '''\n",
    "    \n",
    "    Y_predicted = []\n",
    "    for i in range(3): \n",
    "        model.set_parameter(parameters[i])\n",
    "        model.fit(list_Kernel_train[i], Y_train[i])\n",
    "        Y_predicted.append(model.predict(list_Kernel_test[i])*1)\n",
    "        print('Model {} Predicted'.format(i))\n",
    "\n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM'+name_dossier+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 Predicted\n",
      "Model 1 Predicted\n",
      "Model 2 Predicted\n"
     ]
    }
   ],
   "source": [
    "best_parameters = [0.2057,1.1275,0.06903]\n",
    "download_results(KRR_spectrum_7,list_K_train_spectrum_7,list_K_test_spectrum_7,best_parameters,'K_spectrum_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloads_from_voting(models, lists_Kernel_train,lists_Kernel_test,parameterss, name_dossier): \n",
    "    '''\n",
    "    Do the same as download_result but takes more models and make a prediction based on the prediction of all your models. \n",
    "    \n",
    "    \n",
    "    Takes some models and computes the results by a voting process. \n",
    "    args : \n",
    "            lists_Kernel_train : list of list_Kernel_train.\n",
    "            \n",
    "            lists_Kernel_test : list of list_Kernel_test. \n",
    "\n",
    "            parameterss :list of list of parameter (the best parameter possible). each list of parameters contains 3 parameter : one for each dataset. \n",
    "            \n",
    "            name_dossier :  your predictions will be stocked in the dossier 'predictions_KM'+name_dossier+'.csv'. should be informative\n",
    "            \n",
    "            \n",
    "    returns : None\n",
    "    It automatically downloads the predictions. \n",
    "    \n",
    "    Note that here, if the models disagree and have 50% yes 50% no, then it will be the first model of the list\n",
    "    that will have the last word (works fine for 2 models but not tested for more)\n",
    "    \n",
    "    Maybe we should print the correlation between the models. (To add)\n",
    "    '''\n",
    "    Y_predicted = []\n",
    "    length = len(models)\n",
    "    for i in range(3): \n",
    "        y_pred = list()\n",
    "        for model,list_Kernel_train,list_Kernel_test,parameters in zip(models, lists_Kernel_train, lists_Kernel_test, parameterss): \n",
    "            model.set_parameter(parameters[i])\n",
    "            model.fit(list_Kernel_train[i], Y_train[i])\n",
    "            y_pred.append(model.predict(list_Kernel_test[i]).reshape(-1,1))\n",
    "        y_pred = np.array(np.concatenate(y_pred, axis = 1),float)\n",
    "        y_pred[:,0] = y_pred[:,0]*2-0.5\n",
    "        Y_predicted.append((np.sum(y_pred,axis = 1)>0.5)*1)\n",
    "        \n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM'+name_dossier+'.csv', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [KRR_spectrum_7,KRR_spectrum_7]\n",
    "lists_Kernel_train = [list_K_train_spectrum_7, list_K_train_spectrum_7]\n",
    "lists_Kernel_test = [list_K_test_spectrum_7, list_K_test_spectrum_7]\n",
    "parameterss = [[0.2057,1.1275,0.06903], [0.3636,0.21428,0.2057]]\n",
    "name_dossier = 'voting_two_K_spectrum_7'\n",
    "downloads_from_voting(models, lists_Kernel_train, lists_Kernel_test, parameterss,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_spectrum_traintest[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
