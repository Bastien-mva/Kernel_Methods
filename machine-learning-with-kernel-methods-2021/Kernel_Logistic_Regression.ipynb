{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy.linalg\n",
    "from tqdm import tqdm\n",
    "import numba \n",
    "from numba import njit,vectorize, jit\n",
    "import time \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/Xtr0_mat100.csv', header=None, delimiter = ' ').to_numpy()\n",
    "y_train = pd.read_csv('data/Ytr0.csv')['Bound'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01086957, 0.01086957, 0.04347826, ..., 0.01086957, 0.        ,\n",
       "        0.01086957],\n",
       "       [0.        , 0.        , 0.01086957, ..., 0.0326087 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02173913, 0.01086957, 0.02173913, ..., 0.02173913, 0.02173913,\n",
       "        0.01086957],\n",
       "       ...,\n",
       "       [0.01086957, 0.        , 0.        , ..., 0.0326087 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01086957, 0.01086957, 0.        , ..., 0.        , 0.        ,\n",
       "        0.01086957],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.01086957,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kw': 1}\n"
     ]
    }
   ],
   "source": [
    "def test(required, **kwargs): \n",
    "    print(kwargs)\n",
    "    \n",
    "def other(**kwargs): \n",
    "    test(0,**kwargs)\n",
    "    \n",
    "other(kw = 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def GaussianKernel(x,y,sig2 = 1): \n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))\n",
    "\n",
    "x1 = X_train[0,:]\n",
    "x2 = X_train[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_from_alpha(alpha, Kernel, X):\n",
    "    '''\n",
    "    Calcule f à partir d'alpha. On utilise ici la forumule du representer thm : \n",
    "    f(x) = sum(alpha_i*K(x_i,x))\n",
    "    \n",
    "    args : \n",
    "        alpha : vecteur de taille (nombre de données dans le dataset). \n",
    "        Kernel : n'importe quel kernel \n",
    "        X : Matrice contenant les données. X.shape[0] doit etre eégal à la taille de alpha\n",
    "        \n",
    "    return : la fonction donnée par le representer theorem\n",
    "    '''\n",
    "    \n",
    "    return  lambda x : np.sum([alpha[i]*Kernel(X[i,:],x) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.98475818, 0.98371162, ..., 0.98848832, 0.98942306,\n",
       "        0.98895558],\n",
       "       [0.98475818, 1.        , 0.9799995 , ..., 0.98545651, 0.98173781,\n",
       "        0.97988373],\n",
       "       [0.98371162, 0.9799995 , 1.        , ..., 0.97826428, 0.98196982,\n",
       "        0.97699373],\n",
       "       ...,\n",
       "       [0.98848832, 0.98545651, 0.97826428, ..., 1.        , 0.98708787,\n",
       "        0.98545651],\n",
       "       [0.98942306, 0.98173781, 0.98196982, ..., 0.98708787, 1.        ,\n",
       "        0.98650493],\n",
       "       [0.98895558, 0.97988373, 0.97699373, ..., 0.98545651, 0.98650493,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@njit\n",
    "def to_mat_K(X, Kernel, sig2 = 1): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = X[i,:]\n",
    "        for j in range(i,length): \n",
    "            x_j = X[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K     \n",
    "to_mat_K(X_train,GaussianKernel, sig2 = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0\n",
    "alpha = np.ones(X_train.shape[0])\n",
    "mat_K = to_mat_K(X_train,GaussianKernel, 1)\n",
    "alpha_init = np.ones(mat_K.shape[0])/mat_K.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.97758476e-19, -2.88831459e-19,  1.03389519e-18, ...,\n",
       "        1.31838984e-19, -1.38777878e-19,  4.16333634e-20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize(K): \n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)\n",
    "\n",
    "np.mean(standardize(mat_K), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente quelques fonctions. Ici, on fait simplement une descente de gradient pour commencer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3597433867106116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.91108501701223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@vectorize\n",
    "def loss(u): \n",
    "    return np.log(1+np.exp(-u))\n",
    "def sigmoid(u): \n",
    "    return 1/(1+np.exp(-u))\n",
    "\n",
    "\n",
    "def grad_loss(u): \n",
    "    return -sigmoid(-u)\n",
    "\n",
    "def hess_loss(u): \n",
    "    return sigmoid(u)*sigmoid(-u)\n",
    "\n",
    "def J(alpha, y = y_train, mat_K = mat_K, lam = lam):\n",
    "    n = alpha.shape[0]\n",
    "    regularizer = lam/2*alpha@mat_K@alpha\n",
    "    vect = mat_K@alpha\n",
    "    somme = 1/n*np.sum(loss(y*vect))\n",
    "    return somme+regularizer\n",
    "   \n",
    "def grad_J(alpha, y = y_train, mat_K = mat_K, lam = lam): \n",
    "    n = y.shape[0]\n",
    "    vect_P_alpha = grad_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K@(vect_P_alpha*y)+ lam*mat_K@alpha\n",
    "\n",
    "def hess_J(alpha, y = y_train, mat_K = mat_K, lam = lam):\n",
    "    n = mat_K.shape[0]\n",
    "    vect_W = hess_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K +lam*mat_K\n",
    "\n",
    "def Kernel_logistic_reg_fit(X= X_train, y = y_train, mat_K = mat_K, lam = lam, Niter =400):\n",
    "    alpha = 0.001*np.random.randn(X.shape[0])\n",
    "    #alpha = np.ones(2000)\n",
    "    #mat_K = standardize(mat_K)\n",
    "    lr = 0.2\n",
    "    for i in range(Niter): \n",
    "        #print('alpha :', alpha)\n",
    "        #print('grad : ', grad_J(alpha,mat_K = mat_K))\n",
    "        alpha-= lr*grad_J(alpha,mat_K = mat_K)#, mat_K= mat_K)\n",
    "        #print('J ##########:', J(alpha))\n",
    "    print(J(alpha)-lam/2*alpha@mat_K@alpha)\n",
    "    return alpha\n",
    "lam = 0\n",
    "first_alpha = Kernel_logistic_reg_fit()\n",
    "f = f_from_alpha(first_alpha, GaussianKernel, X_train)\n",
    "f(X_train[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 301 ms, total: 1.48 s\n",
      "Wall time: 255 ms\n",
      "test : 0.2557912422635127\n",
      "test : 0.7252261043031467\n",
      "test : 0.8535851730994182\n",
      " loss without regularization :  0.0357\n",
      "regularization : 0.0378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0734819399593179"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1e-8 marche bien pour lambda\n",
    "def fit_KRR(mat_K,lam,y):\n",
    "    #mat_K = standardize(mat_K) #marche pas si on standardise \n",
    "    n = mat_K.shape[0]\n",
    "    full_mat = mat_K +n*lam*np.eye(n)\n",
    "    alpha = np.linalg.solve(full_mat,y)\n",
    "    return alpha \n",
    "lam = 1e-8\n",
    "%time alpha_KRR = fit_KRR(mat_K,lam,y_train)\n",
    "f_KRR = f_from_alpha(alpha_KRR, GaussianKernel, X_train)\n",
    "print('test :', f_KRR(X_train[0,:]))\n",
    "print('test :', f_KRR(X_train[1,:]))\n",
    "print('test :', f_KRR(X_train[2,:]))\n",
    "evaluate_MSE_from_alpha(alpha_KRR,X_train,y_train,lam, mat_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 474 ms, sys: 17.6 ms, total: 491 ms\n",
      "Wall time: 213 ms\n"
     ]
    }
   ],
   "source": [
    "# meme fonction que WKRR, mais on est environ 10 fois plus lent\n",
    "'''\n",
    "def WKRR(mat_K,vect_W, lam, y): \n",
    "    # pour l'instant on suppose que W est bien inversible, i.e. aucune valeur à zéro\n",
    "    n = mat_K.shape[0]\n",
    "    mat_sqrt_W = np.diag(np.sqrt(vect_W))\n",
    "    mat_neg_sqrt_W = np.diag(1/np.sqrt(vect_W))\n",
    "    big_mat = mat_sqrt_W@mat_K@mat_sqrt_W + n*lam*np.eye(n)\n",
    "    return scipy.linalg.solve(big_mat@mat_neg_sqrt_W,mat_sqrt_W@y)\n",
    "'''\n",
    "\n",
    "\n",
    "vect_W_init = np.ones(mat_K.shape[0])\n",
    "def fit_WKRR(mat_K,vect_W,lam,y): \n",
    "    '''\n",
    "    Compute the Weighted Kernel Redge Regression. the Formula is given in the course. \n",
    "    The code is optimized, we do not take the diagonal matrix of the square root of W. Instead, \n",
    "    we only compute some np.multiply stuff. \n",
    "    \n",
    "    args : \n",
    "    \n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            vect_W : the vector that contains the weight associated to each sample. here we need that all the \n",
    "            coefficient of this vector is 0. Otherwise we won't be able to compute the inverse of the square root\n",
    "            lam : regularization factor \n",
    "            y : the vector we train on \n",
    "    \n",
    "    returns :\n",
    "            \n",
    "            the vector alpha that satisfy the formula in the course. \n",
    "    alpha then needs to be transformed to a function in order to fit the data.\n",
    "    '''\n",
    "    min_W = np.min(vect_W)\n",
    "    if (min_W < 0) or (min_W == 0) : \n",
    "        print('Non invertible Matrix W ')\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    n = mat_K.shape[0]\n",
    "    vect_sqrt_W = np.sqrt(vect_W) # the square root of the original vector\n",
    "    vect_neg_sqrt_W = 1/vect_sqrt_W # the negative square root of the original vector\n",
    "    \n",
    "    b = np.multiply(vect_sqrt_W,y_train) \n",
    "    #here we compute the matrix that needs to be inverted. We just compute it and will solve a linear system \n",
    "    #instead of computing the inverse (more efficient)\n",
    "    big_mat = np.multiply(np.multiply(vect_sqrt_W.reshape(-1,1),mat_K), vect_sqrt_W) +n*lam*np.eye(n)\n",
    "    A = np.multiply(vect_neg_sqrt_W,big_mat)\n",
    "    return scipy.linalg.solve(A,b)\n",
    "\n",
    "\n",
    "%time alpha_WKRR = fit_WKRR(mat_K,vect_W_init,lam,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator(): \n",
    "    def __init__(self , Kernel = GaussianKernel, lam = 1e-8, sig2 = 1 ): \n",
    "        self.Kernel = Kernel\n",
    "        self.lam = lam \n",
    "        self.sig2 = sig2 \n",
    "        self.mat_K = None \n",
    "        self.alpha = None \n",
    "        self.f = None \n",
    "        \n",
    "    def predict_proba(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            probs = np.empty(X.shape[0])\n",
    "            for i in range(X.shape[0]): \n",
    "                probs[i] = self.f(X[i,:])\n",
    "            return probs \n",
    "    \n",
    "    \n",
    "    def predict(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(X)\n",
    "            return prob>0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lam = 1e-8 est bien\n",
    "class KRR(): \n",
    "    def __init__(self , Kernel = GaussianKernel, lam = 1e-8, sig2 = 1 ): \n",
    "        self.Kernel = Kernel\n",
    "        self.lam = lam \n",
    "        self.sig2 = sig2 \n",
    "        self.mat_K = None \n",
    "        self.alpha = None \n",
    "        self.f = None \n",
    "        #self.vect_W = vect_W \n",
    "    def fit(self, X, y): \n",
    "        if self.Kernel == GaussianKernel : \n",
    "            self.mat_K = to_mat_K(X, self.Kernel,self.sig2)\n",
    "        self.alpha = fit_KRR(self.mat_K, self.lam, y)\n",
    "        self.f = f_from_alpha(self.alpha,self.Kernel,X)\n",
    "    \n",
    "    def predict_proba(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            probs = np.empty(X.shape[0])\n",
    "            for i in range(X.shape[0]): \n",
    "                probs[i] = self.f(X[i,:])\n",
    "            return probs \n",
    "    def predict(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(X)\n",
    "            return prob>0.5\n",
    "        \n",
    "# test = KRR(sig2=1)\n",
    "# test.fit(X_train,y_train)\n",
    "# np.sum(np.abs(test.predict(X_train)-y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "class KRL(estimator): \n",
    "    def __init__(self , Kernel = GaussianKernel, \n",
    "                 lam = 1e-8 , sig2 = 1): \n",
    "        super().__init__(Kernel, lam, sig2)\n",
    "        \n",
    "    def fit(self,X,y,max_iter = 10): \n",
    "        if self.Kernel == GaussianKernel: \n",
    "            self.mat_K = to_mat_K(X,self.Kernel,self.sig2)\n",
    "        vect_W_init = np.ones(self.mat_K.shape[0])\n",
    "        self.alpha = fit_KLR_IRLS(self.mat_K, self.lam, y,vect_W_init,max_iter)\n",
    "        self.f = f_from_alpha(self.alpha,self.Kernel,X)\n",
    "        \n",
    "test = KRL()\n",
    "test.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss without regularization :  0.0\n",
      "regularization : 0.0\n",
      "WKRR : 1.7515959306586573e-08\n"
     ]
    }
   ],
   "source": [
    "def evaluate_MSE_from_alpha(alpha,X,y,lam,mat_K, Kernel = GaussianKernel):\n",
    "    '''\n",
    "    Function that computes the MSE of the vector computed alpha. \n",
    "    \n",
    "    args : \n",
    "            alpha : this is the final value we compute. We do not look directly for a function but for some \n",
    "            parameter that will completely determined the function. alpha is this parameter\n",
    "            X : training data \n",
    "            y : target data \n",
    "            lam : regularization factor\n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            Kernel : the kernel we are using. Normally, mat_K has been computed with the kernel K\n",
    "            \n",
    "    returns : \n",
    "            the MSE of the data plus the regularization factor\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    f_alpha = f_from_alpha(alpha,Kernel,X)\n",
    "    loss = 0\n",
    "    for i in range(n): \n",
    "        loss+= (y[i]-f_alpha(X[i,:]))**2.0\n",
    "    loss/= n\n",
    "    print(' loss without regularization : ', np.round(loss,4)) \n",
    "    reg = lam*alpha@mat_K@alpha\n",
    "    print('regularization :', np.round(reg,4))\n",
    "    return loss + reg \n",
    "\n",
    "\n",
    "print('WKRR :',evaluate_MSE_from_alpha(alpha_WKRR, X_train, y_train, lam, mat_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.1484107417636551\n",
      "test : 0.820260876629618\n",
      "test : 0.9131023120571626\n",
      " loss without regularization :  0.0158\n",
      "regularization : 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022552382939882546"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_KLR_IRLS(mat_K, lam, y,  vect_W = vect_W_init, max_iter = 10): \n",
    "    alpha_init = 0.0001*np.random.randn(mat_K.shape[0])\n",
    "    m_t = mat_K@alpha_init\n",
    "    sigmoid_negy_m = sigmoid(np.multiply(-y,m_t))\n",
    "    P_t = -sigmoid(sigmoid_negy_m)\n",
    "    W_t = np.multiply(sigmoid(m_t),sigmoid(-m_t))\n",
    "    z_t = m_t + y/sigmoid_negy_m\n",
    "    for i in tqdm(range(max_iter)): \n",
    "        alpha_t = fit_WKRR(mat_K, W_t, lam, z_t)\n",
    "        m_t = mat_K@alpha_t\n",
    "        sigmoid_negy_m = sigmoid(np.multiply(-y,m_t))\n",
    "        P_t = -sigmoid(sigmoid_negy_m)\n",
    "        W_t = np.multiply(sigmoid(m_t),sigmoid(-m_t))\n",
    "        z_t = m_t + y/sigmoid_negy_m\n",
    "        #print(J(alpha_t))\n",
    "    return alpha_t\n",
    "        \n",
    "lam = 1e-9\n",
    "alpha_KLR = fit_KLR_IRLS(mat_K,lam,y_train)\n",
    "f_KLR = f_from_alpha(alpha_KLR, GaussianKernel,X_train)\n",
    "print('test :', f_KLR(X_train[0,:]))\n",
    "print('test :', f_KLR(X_train[1,:]))\n",
    "print('test :', f_KLR(X_train[2,:]))\n",
    "evaluate_MSE_from_alpha(alpha_KLR, X_train,y_train,lam, mat_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss without regularization :  0.0158\n",
      "regularization : 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02255238294801606"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_MSE_from_alpha(alpha_KRL, X_train,y_train,lam, mat_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(vect_W_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
