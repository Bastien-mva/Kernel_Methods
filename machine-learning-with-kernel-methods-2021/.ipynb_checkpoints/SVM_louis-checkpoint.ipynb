{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "humanitarian-investigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qpsolvers\n",
      "  Downloading qpsolvers-1.5.tar.gz (10 kB)\n",
      "Collecting quadprog>=0.1.8\n",
      "  Downloading quadprog-0.1.8.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 619 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Cython\n",
      "  Using cached Cython-0.29.22-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Building wheels for collected packages: qpsolvers, quadprog\n",
      "  Building wheel for qpsolvers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for qpsolvers: filename=qpsolvers-1.5-py3-none-any.whl size=18784 sha256=573ab61b01bf5d4cf3fcc394637ea2e18f27606bb498dfc2400ff4f494b65272\n",
      "  Stored in directory: /home/bastien/.cache/pip/wheels/f1/ec/63/6cf0699e79ab55a076e8102b075cb9967eda37dc18ec300c6b\n",
      "  Building wheel for quadprog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for quadprog: filename=quadprog-0.1.8-cp38-cp38-linux_x86_64.whl size=540821 sha256=388d5d1bf306665da24882799edc67b8efef0e03094bc1de01cbc6b1d06eec5a\n",
      "  Stored in directory: /home/bastien/.cache/pip/wheels/b7/9a/ae/8b1455b942197ab2631f74f0e47dafa1cc1c3aeea28d8953c4\n",
      "Successfully built qpsolvers quadprog\n",
      "Installing collected packages: Cython, quadprog, qpsolvers\n",
      "Successfully installed Cython-0.29.22 qpsolvers-1.5 quadprog-0.1.8\n",
      "Collecting cvxopt\n",
      "  Downloading cvxopt-1.2.6-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.2.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install qpsolvers\n",
    "#!pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "little-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg\n",
    "from tqdm import tqdm\n",
    "import numba \n",
    "from numba import njit, vectorize, jit\n",
    "import time \n",
    "import scipy\n",
    "\n",
    "import qpsolvers\n",
    "from qpsolvers import solve_qp\n",
    "from qpsolvers import dense_solvers, sparse_solvers\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import cvxopt\n",
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "official-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour récupérer toutes les données\n",
    "X_train0 = pd.read_csv('data/Xtr0_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train0 = pd.read_csv('data/Ytr0.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test0 = pd.read_csv('data/Xte0_mat100.csv',delimiter= ' ', header= None).values\n",
    "\n",
    "X_train1 = pd.read_csv('data/Xtr1_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train1 = pd.read_csv('data/Ytr1.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test1 = pd.read_csv('data/Xte1_mat100.csv',delimiter= ' ', header= None).values\n",
    "\n",
    "X_train2 = pd.read_csv('data/Xtr2_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train2 = pd.read_csv('data/Ytr2.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test2 = pd.read_csv('data/Xte2_mat100.csv',delimiter= ' ', header= None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "straight-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def GaussianKernel(x, y, sig2 = 1): \n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))\n",
    "@njit\n",
    "def Linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "@njit\n",
    "def Polynomial_kernel(x, y, p=5):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "@njit\n",
    "def Laplace_kernel(x, y, gamma=1):\n",
    "    return 0.5 * np.exp(-gamma * numpy.linalg.norm(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def to_Kernel_train(X, Kernel, sig2 = 1): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = X[i,:]\n",
    "        for j in range(i,length): \n",
    "            x_j = X[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel,sig2=1):\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in range(length_train):\n",
    "        x_i = Xtrain[i,:]\n",
    "        for j in range(length_test): \n",
    "            x_j = Xtest[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            bimat_K[i,j] = value\n",
    "    return bimat_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "domestic-sustainability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 2.33 ms, total: 1.15 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%time Kernel_train = to_Kernel_train(X_train0,GaussianKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cooperative-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_split(Xtrain, ytrain, cv):\n",
    "    idx = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(idx) # we shuffle the indices to get random samples\n",
    "    sample_size = Xtrain.shape[0]//cv\n",
    "    Xtrainsplit = []# a list that wil contain each X_train vector. Each element will be smaller than \n",
    "                    #X_train. If cv = 3 for example, the size (on the x axis) will be 2/3 the original size \n",
    "    ytrainsplit = []\n",
    "    Xvalsplit = []\n",
    "    yvalsplit = []\n",
    "    for i in range(cv-1): \n",
    "        #we add the new indices. Here, takes the original vector and returns the vector without the \n",
    "        # indices passes in argument \n",
    "        Xtrainsplit.append(np.delete(Xtrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        ytrainsplit.append(np.delete(ytrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        \n",
    "        # we add the rest \n",
    "        # note that here, we keep the same labels for X ( we do not shuffle independantly X and y)\n",
    "        Xvalsplit.append( Xtrain[idx[i*sample_size:(i+1)*sample_size],:])\n",
    "        yvalsplit.append(ytrain[idx[i*sample_size:(i+1)*sample_size]])\n",
    "    # we add the last round. It is different since we can't take float proportion of an array, \n",
    "    # we have to take an integer. So, here we just add what remains. \n",
    "    Xtrainsplit.append(np.delete(Xtrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    ytrainsplit.append(np.delete(ytrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    Xvalsplit.append( Xtrain[idx[(cv-1)*sample_size:],:])\n",
    "    yvalsplit.append(ytrain[idx[(cv-1)*sample_size:]])\n",
    "    return Xtrainsplit,Xvalsplit,ytrainsplit,yvalsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exotic-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_results(model, Kernel, c0, c1, c2):\n",
    "    model0 = SVM_1(Kernel , C=c0)\n",
    "    model0.fit(X_train0, Y_train0)\n",
    "    Y_test0 = model.predict(X_test0)\n",
    "    \n",
    "    print('First Model Predicted')\n",
    "    model1 = SVM_1(Kernel, C=c1)\n",
    "    model1.fit(X_train1, Y_train1)\n",
    "    Y_test1 = model.predict(X_test1)\n",
    "    \n",
    "    print('Second Model Predicted')\n",
    "    model2 = SVM_1(Kernel, C=c2)\n",
    "    model2.fit(X_train2, Y_train2)\n",
    "    Y_test2 = model.predict(X_test2)\n",
    "    print('Finised')\n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate([Y_test0, Y_test1, Y_test2])}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-external",
   "metadata": {},
   "source": [
    "J'ai fais 2 SVM avec deux technique différentes, et ils donnent des résultats différent, SVM_1 j'ai remplacé alpha*Y pas beta et minimisé par rapport à beta, et SVM_1 c'est le normal comme la formule du cours. Je pense que SVM_2 a des résultats plus logiques, faut prendre C=5 environ.\n",
    "\n",
    "Au début j'ai voulu faire la méthode du cours mais rien marchait, je me suis donc inspiré d'un truc sur github où ils utilisaient le changement de variable avec beta donc j'ai fais ça, c'est celui là qui fonctionne bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "knowing-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Kernel)\n",
    "\n",
    "x_train = X_train2[:1500]\n",
    "y_train = Y_train2[:1500]\n",
    "x_val = X_train2[1500:]\n",
    "y_val = Y_train2[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "homeless-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec cvxopt pour résoudre min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C  \n",
    "#(en gros b= alpha * diag(Y))\n",
    "class estimator(): \n",
    "    def __init__(self , Kernel, lam = 1e-8, sig2 = 1 ): \n",
    "        self.Kernel = Kernel\n",
    "        self.lam = lam \n",
    "        self.sig2 = sig2 \n",
    "        self.mat_K = None \n",
    "        self.alpha = None \n",
    "        self.b = 0 \n",
    "        \n",
    "    def predict_proba(self,Xtest): \n",
    "        if (self.alpha == None).any()==True  : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            mat_K_test = to_bimat_K(self.X_train,Xtest,self.Kernel)\n",
    "            return  sigmoid(self.alpha@mat_K_test)\n",
    "    \n",
    "    def predict(self,Xtest): \n",
    "        if (self.alpha == None).any()==True : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(Xtest)\n",
    "            return prob>0.5\n",
    "    def cross_val(self, Xtrain,ytrain,cv): \n",
    "        mistake = 0\n",
    "        Xtrainsplit,Xvalsplit,ytrainsplit,yvalsplit = cross_val_split(Xtrain,ytrain,cv)\n",
    "        for xtrain,xval,ytrain,yval in tqdm(zip(Xtrainsplit,Xvalsplit, ytrainsplit, yvalsplit)):\n",
    "            self.fit(xtrain,ytrain)\n",
    "            pred = self.predict(xval)\n",
    "            mistake+=np.sum(np.abs(pred-yval))\n",
    "        print('Pourcentage of errors : ', mistake/Xtrain.shape[0])\n",
    "        return mistake/Xtrain.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SVM_1():\n",
    "    def __init__(self, Kernel, C = 1):\n",
    "        self.kernel = Kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        print('Initialized')\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        print('On rentre dans le fit ')\n",
    "        n = len(Y)\n",
    "        #calculate the kernel\n",
    "        #K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "        K = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "        \n",
    "\n",
    "        lbd = 1\n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        #take Y as -1 and 1\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        \n",
    "        P = cvxopt.matrix(np.outer(label, label) * K ) \n",
    "        q = cvxopt.matrix(-np.ones(n))\n",
    "        A = cvxopt.matrix(label, (1,n), 'd')\n",
    "        b = cvxopt.matrix(0.0)\n",
    "                \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        ça revient au même et je crois que le solver devrait fonctionner avec ça, mais j'y arrive pas encore\n",
    "        '''\n",
    "        # b <= C\n",
    "        G1 = np.eye(n)\n",
    "        h1 = np.ones(n) * self.C\n",
    "        \n",
    "        # -b <= 0\n",
    "        G2 = -np.eye(n)\n",
    "        h2 = np.zeros(n)\n",
    "        \n",
    "        G = cvxopt.matrix(np.vstack((G2, G1)))\n",
    "        h = cvxopt.matrix(np.hstack((h2, h1)))\n",
    "\n",
    "        #min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C\n",
    "        solver = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        self.all_alpha = np.ravel(solver['x']) # le alpha ou on garde toutes les coordonnées \n",
    "                                                # en comparaison avec le self.alpha ou en garde que quelques-uns \n",
    "        \n",
    "        #Je retire les vecteurs avec un alpha trop petit\n",
    "        eps = 1e-5\n",
    "        supportIndices = self.all_alpha > eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        self.support_vectors = X[supportIndices]\n",
    "        self.support_Y = label[supportIndices]\n",
    "        self.alpha = self.all_alpha[supportIndices]  #alpha : all_alpha sans les alpha < eps\n",
    "        print('We keep ', len(self.alpha), 'support vectors out of',len(self.all_alpha),'vectors')\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alpha)):\n",
    "            self.b = self.support_Y[i]\n",
    "            self.b -= np.sum( self.alpha * self.support_Y * K[ind[i], supportIndices])\n",
    "        self.b /= len(self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_predict = np.zeros(len(X))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            print('alpha :', self.alpha )\n",
    "            print('supp vectors : ', self.support_vectors)\n",
    "            print('support y ', self.support_Y)\n",
    "            for alpha, sv, label in zip(self.alpha, self.support_vectors, self.support_Y):\n",
    "                y_predict[i] += alpha * label * self.kernel(sv, X[i]) \n",
    "        \n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "        #return y_predict + self.b\n",
    "    \n",
    "    def cross_val(self, Xtrain, ytrain, cv): \n",
    "        mistake = 0\n",
    "        Xtrainsplit, Xvalsplit, ytrainsplit, yvalsplit = cross_val_split(Xtrain, ytrain, cv)\n",
    "        for xtrain, xval, ytrain, yval in tqdm(zip(Xtrainsplit, Xvalsplit, ytrainsplit, yvalsplit)):\n",
    "            self.fit(xtrain, ytrain)\n",
    "            pred = self.predict(xval)\n",
    "            print('Accuracy :', 1 - np.sum(np.abs(pred - yval)) / pred.shape[0])\n",
    "            mistake += np.sum(np.abs(pred - yval))\n",
    "        print('Average accuracy: ', 1 - mistake / Xtrain.shape[0])\n",
    "        return 1 - mistake / Xtrain.shape[0]\n",
    "    \n",
    "model = SVM_1(Kernel = GaussianKernel, C=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "veterinary-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_C(X, Y, C_list, Kernel, cv = 4):\n",
    "    acc = []\n",
    "    for c in C_list:\n",
    "        print('____ For C = ', c, '____')\n",
    "        model = SVM_1(Kernel, C=c)\n",
    "        acc.append(model.cross_val(X, Y, cv))\n",
    "    best_param = C_list[np.argmax(acc)]\n",
    "    print('The best value for C is', best_param, 'we get', np.max(acc))\n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "blessed-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1939 support vectors out of 2000 vectors\n",
      "alpha : None\n",
      "supp vectors :  None\n",
      "support y  None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c830ff84a9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdowload_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGaussianKernel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-1457b603bece>\u001b[0m in \u001b[0;36mdowload_results\u001b[0;34m(model, Kernel, c0, c1, c2)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mY_test0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'First Model Predicted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6b618935b3f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'supp vectors : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'support y '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "dowload_results(model,GaussianKernel,  0.003, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "threaded-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____ For C =  1 ____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [01:27, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1488 support vectors out of 1500 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SVM_1' object has no attribute 'alpha_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a22d121a325f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolynomial_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-22145e4a53c2>\u001b[0m in \u001b[0;36mgrid_search_C\u001b[0;34m(X, Y, C_list, Kernel, cv)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'____ For C = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'____'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The best value for C is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'we get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-546e489b46b0>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(self, Xtrain, ytrain, cv)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrainsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXvalsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrainsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalsplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mmistake\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-546e489b46b0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVM_1' object has no attribute 'alpha_'"
     ]
    }
   ],
   "source": [
    "best_c = grid_search_C(X_train1, Y_train1, [1,0.1,0.01], Kernel = Polynomial_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "expanded-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1304 support vectors.\n",
      "[1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0] [0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
      " 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1]\n",
      "accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "model = SVM_1(kernel = GaussianKernel, C=5)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "results = model.predict(x_val)\n",
    "\n",
    "print( results[:100], y_val[:100])\n",
    "n = results == y_val\n",
    "print('accuracy:', sum(n)/len(results))\n",
    "#print(model.alpha, len(model.alpha_), model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-circuit",
   "metadata": {},
   "source": [
    "### Autre SVM (pas besoin car SVM_1 fonctionne très bien):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "amended-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec cvxopt pour résoudre min_a 1/2 * alpha.T * K * alpha - alpha.T * Y s.t. 0<=y*alpha<=C\n",
    "class SVM_2:\n",
    "    def __init__(self, kernel = GaussianKernel, C = 1):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        n = len(Y)        \n",
    "        #K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "                \n",
    "        K = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        #lbd = 1\n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        # P=K et q=-Y\n",
    "        P = cvxopt.matrix(K) \n",
    "        q = cvxopt.matrix(-label, tc='d')\n",
    "                \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        '''\n",
    "        \n",
    "        # Condition 0 <= alpha_i * Y_i <= C\n",
    "        G1 = np.diag(-label)\n",
    "        G2 = np.diag(label)\n",
    "        G = cvxopt.matrix(np.vstack((G1, G2)), tc='d')\n",
    "        \n",
    "        h1 = np.zeros((n, 1), dtype='float64')\n",
    "        h2 = self.C * np.ones((n, 1), dtype='float64')\n",
    "        h = cvxopt.matrix(np.vstack((h1, h2)))\n",
    "\n",
    "        # solves min_a 1/2 a^T * P * a + q^T * a s.t. G*a <= h\n",
    "        solver = cvxopt.solvers.qp(P, q, G, h)\n",
    "        \n",
    "        self.alpha = np.ravel(solver['x'])\n",
    "        \n",
    "        #Je retire les vecteurs avec un alpha trop petit\n",
    "        eps = 1e-5\n",
    "        supportIndices = np.abs(self.alpha) > eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        self.support_vectors = X[supportIndices]\n",
    "        self.support_Y = label[supportIndices]\n",
    "        self.alpha_ = self.alpha[supportIndices]  #alpha_ : alpha sans les alpha < eps\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alpha_)):\n",
    "            self.b = self.support_Y[i]\n",
    "            self.b -= np.sum( self.alpha_ * K[ind[i], supportIndices])\n",
    "        self.b /= len(self.alpha_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_predict = np.zeros(len(X))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(self.alpha_)):\n",
    "                y_predict[i] += self.alpha_[j] * self.kernel(self.support_vectors[j], X[i])\n",
    "        \n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "        #return y_predict + self.b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "novel-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM_2(kernel = GaussianKernel, C=25)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "results = model.predict(x_val)\n",
    "\n",
    "print( results[:100], y_val[:100])\n",
    "n = results == y_val\n",
    "print(sum(n))\n",
    "print(model.alpha, len(model.alpha_), model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-protein",
   "metadata": {},
   "source": [
    "EN dessous il y a un autre avec la même méthode que 'SVM_1' mais avec un autre optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beginning-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avec qp_solver pour résoudre min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C  \n",
    "#(en gros b= alpha * diag(Y))\n",
    "\n",
    "#Il marche et donne le même résultat que cvxopt\n",
    "\n",
    "class SVM_autre:\n",
    "    def __init__(self, kernel=GaussianKernel, C=1):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        n = len(Y)        \n",
    "        K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "        \n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        P = np.outer(label, label) * K\n",
    "        q = - np.ones(n)\n",
    "        \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        ça revient au même et je crois que le solver devrait fonctionner avec ça, mais j'y arrive pas encore\n",
    "        '''\n",
    "        G = np.vstack((np.eye(n), -np.eye(n)))\n",
    "\n",
    "        h = np.ones(2*n)\n",
    "        h[:n] = h[:n] * self.C\n",
    "        h[n:] = h[n:] * 0\n",
    "\n",
    "        A = label\n",
    "        b = np.array([0.])\n",
    "\n",
    "        self.alpha = solve_qp(P, q.astype('double'), G.astype('double'), h, A, b, solver = 'quadprog')\n",
    "        \n",
    "        #Pour le moment je garde tout X mais faudrait retirer les X dont le alpha est trop bas\n",
    "        eps = 1e-15\n",
    "        supportIndices = self.alpha > eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        self.support_vectors = X[supportIndices]\n",
    "        self.support_Y = label[supportIndices]\n",
    "        self.alpha_ = self.alpha[supportIndices]\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alpha_)):\n",
    "            self.b = self.support_Y[i]\n",
    "            self.b -= np.sum( self.alpha_ * self.support_Y * K[ind[i], supportIndices])\n",
    "        self.b /= len(self.alpha_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        y_predict = np.zeros(len(X))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            for alpha, sv, label in zip(self.alpha_, self.support_vectors, self.support_Y):\n",
    "                y_predict[i] += alpha * label * self.kernel(sv, X[i]) \n",
    "\n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "    \n",
    "    def predict_bis(self, X):\n",
    "        \n",
    "        def predict_one(x):\n",
    "            pred = np.apply_along_axis(lambda s: self.kernel(s, x), 1, self.support_vectors)\n",
    "            pred = pred * self.support_Y * self.alpha\n",
    "            return np.sum(pred)\n",
    "        \n",
    "        preds = np.apply_along_axis(predict_one, 1, X)\n",
    "        return 1 * (preds > 0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "related-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm = SVM_autre(kernel = GaussianKernel, C=0.01)\n",
    "svm.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aerial-fitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1] [0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0]\n",
      "[0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00061584\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.00433891 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00264192 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00251599\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.00961544 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00032761 0.0042436  0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00226446 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.00452394 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.0040861\n",
      " 0.01       0.01       0.00210576 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00224987\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.00870044 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.00177014 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01      ]\n"
     ]
    }
   ],
   "source": [
    "results = svm.predict(x_val)\n",
    "print(results[:100], y_val[:100])\n",
    "print(svm.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-pharmaceutical",
   "metadata": {},
   "source": [
    "Aide svm github : https://github.com/zongmianli/mva-kernel-methods/blob/kernel-challenge/svm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "tired-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02414692  0.00498681  0.01534296  0.03327065  0.02499289  0.01782278\n",
      "  0.00641423  0.01737927  0.00571243  0.03785679  0.02937389 -0.0020743\n",
      "  0.01801311  0.01202195  0.02616614  0.02493651  0.02069579  0.00564455\n",
      " -0.00066408  0.02225113  0.00237909  0.03542867  0.00787691  0.02247312\n",
      "  0.02751154  0.00633758  0.01760011  0.0165434   0.0267509   0.03206684\n",
      "  0.01438477  0.03242849  0.01270201  0.01455076  0.00451208  0.01183673\n",
      "  0.01354231  0.00326173  0.04398204  0.02790272 -0.00445432  0.04707981\n",
      "  0.00802263  0.02043321  0.00878177  0.01557869  0.03094886  0.03406918\n",
      "  0.02125684  0.02878366  0.00324311  0.01825782  0.02390436  0.00269406\n",
      "  0.02602237  0.01234005  0.00085998  0.0158754   0.02489576  0.02026007\n",
      "  0.00903557  0.0151773   0.03625054  0.01268538  0.00022868  0.00617082\n",
      "  0.0134089   0.01244844 -0.00563212  0.00012831  0.02412231  0.02177587\n",
      "  0.01969552  0.03197964  0.01770224  0.01614737  0.01016502  0.03356141\n",
      "  0.00210028  0.02871426  0.00812573 -0.00372503  0.02350311  0.03760823\n",
      "  0.01724742  0.01470091  0.0336364   0.02919009  0.02509313  0.00788259\n",
      "  0.0071019   0.00382853  0.03857615  0.01813762  0.00941578 -0.00600826\n",
      "  0.02025118  0.0257626   0.0119341  -0.00116485] [1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0] [179.53299291473328, 179.2638193446423, 180.06224292004032, 179.98659862549744, 180.16655562663908, 179.22562567027808, 179.75516827499877, 179.66681955881654, 180.18673038466085, 179.70241921540344, 179.75791492680966, 180.00640157214227, 179.74911107109548, 179.44817542047855, 179.7850907331536, 179.5778626028203, 179.5496336538501, 179.8592049366436, 174.77746331094124, 179.59672640457802, 178.35988857531692, 179.8078495329257, 179.31872034329956, 180.07631170977538, 179.578343086164, 179.39241844112905, 180.2470769371597, 179.74626811428834, 179.2629305887961, 179.56186642155092, 180.12031700199594, 179.73210576965843, 179.49982565165803, 179.97592902073393, 179.95039953989735, 179.788659581134, 179.775622431027, 180.20588015176787, 179.3612256550765, 179.7158465553939, 179.2436108196129, 179.31601944226543, 179.26289857068903, 180.05071604001594, 180.0069337865176, 179.98289655308085, 179.79608566334838, 180.04765977314327, 179.84502724551533, 179.88596317123944, 178.68342513554018, 179.87434479060593, 180.02386270412558, 179.6190805627739, 177.7993652170307, 180.01741078269117, 180.094808334175, 180.06057382875395, 179.74661948524238, 179.86970409796655, 179.71353103963804, 180.14250582326866, 179.96612741819962, 180.03915165612278, 179.50714427689144, 180.0398283532452, 179.9683182002998, 179.55131704933473, 179.9211668447428, 179.88145303643572, 179.87976394730816, 179.63064625853465, 179.8714339296601, 179.5202150128781, 178.89877277044604, 179.99463301154293, 179.58198307647763, 179.7409697119273, 179.1261397883086, 179.39255188559272, 179.7519504616132, 179.87882035781223, 180.1244229738326, 179.77478119935347, 180.02566561893596, 179.71273049858195, 179.47839684275363, 179.92173758422285, 179.6568178309584, 180.17560952157513, 179.45673093356072, 179.8877368182291, 179.70823779075891, 180.1933542168511, 179.85672035873722, 179.7460786341896, 179.71641059675997, 179.468887095894, 179.7800981991421, 179.69958266160398]\n"
     ]
    }
   ],
   "source": [
    "#Tests sur les fct de prédiction\n",
    "\n",
    "def predict_one(x):\n",
    "    pred = np.apply_along_axis(lambda s: GaussianKernel(s, x), 1, X_bis)\n",
    "    pred = pred * Y_bis * model.alpha\n",
    "    return np.sum(pred)\n",
    "\n",
    "def f_from_alpha(alpha, Kernel, X):\n",
    "    return  lambda x : np.sum([alpha[i]*Kernel(X[i,:],x) for i in range(X.shape[0])])\n",
    "f_alpha = f_from_alpha(model.alpha, GaussianKernel, x_train)\n",
    "\n",
    "res = [f_alpha(x_val[i]) for i in range(len(y_val))]\n",
    "\n",
    "preds = np.apply_along_axis(predict_one, 1, x_val)\n",
    "#preds = 1 * (preds > 0)\n",
    "print(preds, y_val, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
