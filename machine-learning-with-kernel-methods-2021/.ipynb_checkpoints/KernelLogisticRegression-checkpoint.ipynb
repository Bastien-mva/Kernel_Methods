{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparable-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg \n",
    "from tqdm import tqdm\n",
    "import numba\n",
    "from numba import njit,vectorize, jit\n",
    "import time\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graduate-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/Xtr0_mat100.csv', header=None, delimiter = ' ').to_numpy()\n",
    "X_test = pd.read_csv('data/Xte0_mat100.csv', header=None, delimiter = ' ').to_numpy()\n",
    "y_train = pd.read_csv('data/Ytr0.csv')['Bound'].to_numpy()\n",
    "y_train_rademacher = 2*y_train-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bastien/Documents/ENS/KM/Kernel_Methods/machine-learning-with-kernel-methods-2021\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handmade-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def GaussianKernel(x,y,sig2 = 1):\n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threatened-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def to_Kernel_train(X, Kernel, sig2 = 1): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = X[i,:]\n",
    "        for j in range(i,length): \n",
    "            x_j = X[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel,sig2=1):\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in range(length_train):\n",
    "        x_i = Xtrain[i,:]\n",
    "        for j in range(length_test): \n",
    "            x_j = Xtest[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            bimat_K[i,j] = value\n",
    "    return bimat_K\n",
    "\n",
    "\n",
    "lam = 0\n",
    "sig2 = 1\n",
    "Kernel_train = to_Kernel_train(X_train,GaussianKernel, 1)\n",
    "Kernel_test = to_Kernel_test(X_train,X_test,GaussianKernel)\n",
    "def standardize(K): \n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fuzzy-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@vectorize\n",
    "def loss(u): \n",
    "    return np.log(1+np.exp(-u))\n",
    "def sigmoid(u): \n",
    "    return 1/(1+np.exp(-u))\n",
    "\n",
    "\n",
    "def grad_loss(u): \n",
    "    return -sigmoid(-u)\n",
    "\n",
    "def hess_loss(u): \n",
    "    return sigmoid(u)*sigmoid(-u)\n",
    "\n",
    "def J(alpha, y = y_train, mat_K = Kernel_train, lam = lam):\n",
    "    n = alpha.shape[0]\n",
    "    regularizer = lam/2*alpha@mat_K@alpha\n",
    "    vect = mat_K@alpha\n",
    "    somme = 1/n*np.sum(loss(y*vect))\n",
    "    return somme+regularizer\n",
    "   \n",
    "def grad_J(alpha, y = y_train, mat_K =Kernel_train, lam = lam): \n",
    "    n = y.shape[0]\n",
    "    vect_P_alpha = grad_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K@(vect_P_alpha*y)+ lam*mat_K@alpha\n",
    "\n",
    "def hess_J(alpha, y = y_train, mat_K = Kernel_train, lam = lam):\n",
    "    n = mat_K.shape[0]\n",
    "    vect_W = hess_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K +lam*mat_K\n",
    "\n",
    "def Kernel_logistic_reg_fit(X= X_train, y = y_train, mat_K = Kernel_train, lam = lam, Niter =20):\n",
    "    alpha = 0.00000*np.random.randn(X.shape[0])\n",
    "    #alpha = np.ones(2000)\n",
    "    mat_K = standardize(mat_K)\n",
    "    lr = 5\n",
    "    for i in tqdm(range(Niter)): \n",
    "        #inv = np.linalg.inv(hess_J(alpha, mat_K = mat_K))\n",
    "        #alpha-= lr*inv@grad_J(alpha ,mat_K = mat_K)#, mat_K= mat_K)\n",
    "        alpha-= lr*grad_J(alpha ,mat_K = mat_K)\n",
    "        '''\n",
    "        if i%1 ==0 : \n",
    "            print('alpha :', alpha)\n",
    "            print('J :',J(alpha,mat_K = mat_K))\n",
    "            print('grad :', grad_J(alpha,mat_K = mat_K))\n",
    "    print('alpha_end :', alpha)\n",
    "    print('J_end :',J(alpha,mat_K = mat_K))\n",
    "    print('grad_end :', grad_J(alpha,mat_K = mat_K))'''\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "announced-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1e-8 marche bien pour lambda\n",
    "def fit_KRR(K_train,lam,y):\n",
    "    #mat_K = standardize(mat_K) #marche pas si on standardise \n",
    "    n = K_train.shape[0]\n",
    "    full_mat = K_train +n*lam*np.eye(n)\n",
    "    alpha = np.linalg.solve(full_mat,y)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pending-blond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_KRR = fit_KRR(Kernel_train,lam,y_train)\n",
    "np.max(np.round(alpha_KRR@Kernel_train,3)-y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unlimited-signature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-27046.69790471,  27039.37656427,   9038.83584669, ...,\n",
       "       -17518.73700749,  -9465.23463377,  -5803.08970443])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_WKRR(K_train,vect_W,lam,y): \n",
    "    '''\n",
    "    Compute the Weighted Kernel Redge Regression. the Formula is given in the course. \n",
    "    The code is optimized, we do not take the diagonal matrix of the square root of W. Instead, \n",
    "    we only compute some np.multiply stuff. \n",
    "    \n",
    "    args : \n",
    "    \n",
    "            K_train : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            vect_W : the vector that contains the weight associated to each sample. here we need that all the \n",
    "            coefficient of this vector is 0. Otherwise we won't be able to compute the inverse of the square root\n",
    "            lam : regularization factor \n",
    "            y : the vector we train on \n",
    "    \n",
    "    returns :\n",
    "            \n",
    "            the vector alpha that satisfy the formula in the course. \n",
    "    alpha then needs to be transformed to a function in order to fit the data.\n",
    "    '''\n",
    "    min_W = np.min(vect_W)\n",
    "    if (min_W < 0) or (min_W == 0) : \n",
    "        print('Non invertible Matrix W ')\n",
    "    n = K_train.shape[0]\n",
    "    vect_sqrt_W = np.sqrt(vect_W) # the square root of the original vector\n",
    "    vect_neg_sqrt_W = 1/vect_sqrt_W # the negative square root of the original vector\n",
    "    b = np.multiply(vect_sqrt_W,y) \n",
    "    big_mat = np.multiply(np.multiply(vect_sqrt_W.reshape(-1,1),K_train), vect_sqrt_W) +n*lam*np.eye(n)\n",
    "    A = np.multiply(vect_neg_sqrt_W,big_mat)\n",
    "    return scipy.linalg.solve(A,b)\n",
    "\n",
    "vect_W_init = np.full(Kernel_train.shape[0],1)#/mat_K.shape[0])\n",
    "fit_WKRR(Kernel_train,vect_W_init,lam,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "little-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def IRLS(K, y, alpha):\n",
    "        \"\"\"\n",
    "        Iterative step to update alpha when training the classifier\n",
    "        :param K: np.array, kernel\n",
    "        :param y: np.array, labels\n",
    "        :param alpha: np.array\n",
    "        :return: - W: np.array\n",
    "                 - z: np.array\n",
    "        \"\"\"\n",
    "        m = np.dot(K, alpha)\n",
    "        W = sigmoid(m) * sigmoid(-m)\n",
    "        z = m + y/sigmoid(-y*m)\n",
    "        return W, z\n",
    "\n",
    "def WKRR_af(K, W, z):\n",
    "        \"\"\"\n",
    "        Compute new alpha\n",
    "        :param K: np.array, kernel\n",
    "        :param W: np.array\n",
    "        :param z: np.array\n",
    "        :return: np.array, new alpha\n",
    "        \"\"\"\n",
    "        n = K.shape[0]\n",
    "        W_s = np.diag(np.sqrt(W))\n",
    "        A = np.dot(np.dot(W_s, K), W_s) + n * lam * np.eye(n)\n",
    "        A = np.dot(np.dot(W_s, np.linalg.inv(A)), W_s)\n",
    "        return np.dot(A, z)\n",
    "    \n",
    "    \n",
    "def recoding_KRL(mat_K,lam,y, max_iter = 10): \n",
    "    n = mat_K.shape[0]\n",
    "    old_alpha = 0*np.ones(n)\n",
    "    for i in range(max_iter): \n",
    "        W,z = IRLS(mat_K,y,old_alpha)\n",
    "        alpha = fit_WKRR(mat_K, W, lam, z)\n",
    "        f = f_from_alpha(alpha, GaussianKernel, X_train)\n",
    "        old_alpha = np.copy(alpha)\n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "constitutional-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_m(mat_K,alpha): \n",
    "    return mat_K@alpha\n",
    "\n",
    "def compute_P(y,m): \n",
    "    return -sigmoid(-np.multiply(y,m))\n",
    "\n",
    "def compute_W(m):\n",
    "    return np.multiply(sigmoid(m),sigmoid(m))\n",
    "\n",
    "def compute_z(y,m): \n",
    "    return m + np.multiply(y,1/sigmoid(-np.multiply(y,m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modern-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_KLR_IRLS(mat_K, lam, y, max_iter = 10): \n",
    "    '''\n",
    "    Fonction qui optimise la loss définie par la la Kernel Logistic Regression. \n",
    "    \n",
    "    args : \n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            \n",
    "            lam : regularization factor \n",
    "            \n",
    "            y : the vector we train on. Must be -1 or 1 \n",
    "            \n",
    "            max_iter : the maximum number of iteration we are ready to do \n",
    "    returns : \n",
    "            the vector alpha optimized \n",
    "            alpha then needs to be transformed to a function in order to fit the data.\n",
    "    '''\n",
    "    alpha = np.zeros(mat_K.shape[0])\n",
    "    m = compute_m(mat_K,alpha)    \n",
    "    W = compute_W(m)\n",
    "    z = compute_z(y,m)\n",
    "    for i in range(max_iter): \n",
    "        alpha = fit_WKRR(mat_K,W,lam,z)\n",
    "        m = compute_m(mat_K,alpha)\n",
    "        W = compute_W(m)\n",
    "        z = compute_z(y,m)\n",
    "        f = f_from_alpha(alpha, GaussianKernel, X_train)\n",
    "        for i in range(10): \n",
    "            print(i ,' :',np.round(f(X_train[i,:]),3), ' y :', y_train[i])\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "def cross_val_split(Xtrain,ytrain, cv):\n",
    "    idx = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(idx) # we shuffle the indices to get random samples\n",
    "    sample_size = Xtrain.shape[0]//cv\n",
    "    Xtrainsplit = []# a list that wil contain each X_train vector. Each element will be smaller than \n",
    "                    #X_train. If cv = 3 for example, the size (on the x axis) will be 2/3 the original size \n",
    "    ytrainsplit = []\n",
    "    Xvalsplit = []\n",
    "    yvalsplit = []\n",
    "    for i in range(cv-1): \n",
    "        #we add the new indices. Here, takes the original vector and returns the vector without the \n",
    "        # indices passes in argument \n",
    "        Xtrainsplit.append(np.delete(Xtrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        ytrainsplit.append(np.delete(ytrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        \n",
    "        # we add the rest \n",
    "        # note that here, we keep the same labels for X ( we do not shuffle independantly X and y)\n",
    "        Xvalsplit.append( Xtrain[idx[i*sample_size:(i+1)*sample_size],:])\n",
    "        yvalsplit.append(ytrain[idx[i*sample_size:(i+1)*sample_size]])\n",
    "    # we add the last round. It is different since we can't take float proportion of an array, \n",
    "    # we have to take an integer. So, here we just add what remains. \n",
    "    Xtrainsplit.append(np.delete(Xtrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    ytrainsplit.append(np.delete(ytrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    Xvalsplit.append( Xtrain[idx[(cv-1)*sample_size:],:])\n",
    "    yvalsplit.append(ytrain[idx[(cv-1)*sample_size:]])\n",
    "    return Xtrainsplit,Xvalsplit,ytrainsplit,yvalsplit\n",
    "\n",
    "def Kernel_cross_val_split(K_train,ytrain, cv):\n",
    "    idx = np.arange(K_train.shape[0])\n",
    "    np.random.shuffle(idx) # we shuffle the indices to get random samples\n",
    "    sample_size = K_train.shape[0]//cv\n",
    "    Kerneltrainsplit = []# a list that wil contain each Kernel_train vector. Each element will be smaller than \n",
    "                    #K_train. If cv = 3 for example, the size (on the x axis) will be 2/3 the original size \n",
    "    ytrainsplit = []\n",
    "    Kernelvalsplit = []\n",
    "    yvalsplit = []\n",
    "    for i in range(cv-1): \n",
    "        #we add the new indices. Here, takes the original vector and returns the vector without the \n",
    "        # indices passes in argument \n",
    "        Kerneltrainsplit.append(np.delete(np.delete(K_train,idx[i*sample_size:(i+1)*sample_size],axis = 0),idx[i*sample_size:(i+1)*sample_size],axis = 1))\n",
    "        ytrainsplit.append(np.delete(ytrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        \n",
    "        # we add the rest \n",
    "        # note that here, we keep the same labels for X ( we do not shuffle independantly X and y)\n",
    "        Kernelvalsplit.append(K_train[idx[i*sample_size:(i+1)*sample_size],:][:,idx[i*sample_size:(i+1)*sample_size]])\n",
    "        yvalsplit.append(ytrain[idx[i*sample_size:(i+1)*sample_size]])\n",
    "    # we add the last round. It is different since we can't take float proportion of an array, \n",
    "    # we have to take an integer. So, here we just add what remains. \n",
    "    Kerneltrainsplit.append(np.delete(np.delete(K_train,idx[(cv-1)*sample_size:],axis = 0), idx[(cv-1)*sample_size:], axis = 1))\n",
    "    ytrainsplit.append(np.delete(ytrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    Kernelvalsplit.append( K_train[idx[(cv-1)*sample_size:],:][:,idx[(cv-1)*sample_size:]])\n",
    "    yvalsplit.append(ytrain[idx[(cv-1)*sample_size:]])\n",
    "    return Kerneltrainsplit,Kernelvalsplit,ytrainsplit,yvalsplit\n",
    "\n",
    "Kernel_trainsplit,Kernel_valsplit,y_trainsplit,y_valsplit = Kernel_cross_val_split(Kernel_train,y_train,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thousand-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 1, ..., 1, 0, 0]),\n",
       " array([0, 1, 1, ..., 0, 0, 0]),\n",
       " array([0, 1, 1, ..., 0, 0, 0]),\n",
       " array([0, 1, 1, ..., 0, 0, 0]),\n",
       " array([0, 1, 1, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "traditional-avatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 1995)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(np.delete(Kernel_train,np.arange(0,5), axis = 0),np.arange(0,5), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "running-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator(): \n",
    "    def __init__(self , Kernel): \n",
    "        self.Kernel = Kernel\n",
    "        self.Kernel_train = None \n",
    "        self.alpha = None \n",
    "        \n",
    "    def predict_proba(self,K_test): \n",
    "        if (self.alpha == None).any()==True  : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            print('Kernel_test.shape :', K_test.shape)\n",
    "            return  sigmoid(self.alpha@K_test)\n",
    "    \n",
    "    def predict(self,K_test): \n",
    "        if (self.alpha == None).any()==True : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(K_test)\n",
    "            return prob>0.5\n",
    "    def cross_val(self, K_train,ytrain,cv): \n",
    "        mistake = 0\n",
    "        Kerneltrainsplit,Kernelvalsplit,ytrainsplit,yvalsplit = Kernel_cross_val_split(K_train,ytrain,cv)\n",
    "        for Ktrain,Kval,ytrain,yval in tqdm(zip(Kerneltrainsplit,Kernelvalsplit, ytrainsplit, yvalsplit)):\n",
    "            self.fit(Ktrain,ytrain)\n",
    "            pred = self.predict(Kval)\n",
    "            mistake+=np.sum(np.abs(pred-yval))\n",
    "        print('Pourcentage of errors : ', mistake/K_train.shape[0])\n",
    "        return mistake/K_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "olympic-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lam = 1e-8 est bien\n",
    "class KRR(estimator): \n",
    "    def __init__(self , Kernel, lam = 1e-8): \n",
    "        super(KRR, self).__init__(Kernel)\n",
    "        self.lam = lam \n",
    "        \n",
    "    def fit(self, K_train, y): \n",
    "        y_copy = (2*y-1).copy()\n",
    "        self.Kernel_train = K_train\n",
    "        self.alpha = fit_KRR(self.Kernel_train, self.lam, y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divine-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel_test.shape : (200, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 1800)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c9d0161504c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mKRR_estim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKRR\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mKernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mKRR_estim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f709ebe0e990>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(self, K_train, ytrain, cv)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mKtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerneltrainsplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKernelvalsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrainsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalsplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mmistake\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pourcentage of errors : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmistake\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f709ebe0e990>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, K_test)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Il faut d'abord fitter les données\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f709ebe0e990>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, K_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kernel_test.shape :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mreturn\u001b[0m  \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mK_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 1800)"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1) : \n",
    "    KRR_estim = KRR( Kernel = GaussianKernel, lam = 10**(-13))\n",
    "    KRR_estim.cross_val(Kernel_train,y_train,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = KRR(GaussianKernel)\n",
    "regressor.fit(X_train,y_train_rademacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "encouraging-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(regressor.predict(X_train)-y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "constant-device",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kernel_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "located-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We test with this matrix :  [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[2 3 5 6 7 8]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "[0 1 4]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[0 1 2 4 5 8]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[3 6 7]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[0 1 3 4 6 7]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[5 2 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(3):\\n    print('train : ')\\n    print(X_train_split[i], y_train_split[i])\\n    print('val : ')\\n    print(X_val_split[i],y_val_split[i])\""
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check for the cross_val function \n",
    "'''\n",
    "test = np.eye(9)\n",
    "print('We test with this matrix : ', test)\n",
    "testy = np.arange(9)\n",
    "X_train_split,X_val_split, y_train_split, y_val_split = cross_val_split(test,testy, cv = 3) \n",
    "\n",
    "for i in range(3):\n",
    "    print('train : ')\n",
    "    print(X_train_split[i], y_train_split[i])\n",
    "    print('val : ')\n",
    "    print(X_val_split[i],y_val_split[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "induced-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01086957, 0.01086957, 0.04347826, ..., 0.01086957, 0.        ,\n",
       "        0.01086957],\n",
       "       [0.        , 0.        , 0.01086957, ..., 0.0326087 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02173913, 0.01086957, 0.02173913, ..., 0.02173913, 0.02173913,\n",
       "        0.01086957],\n",
       "       ...,\n",
       "       [0.01086957, 0.        , 0.        , ..., 0.0326087 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01086957, 0.01086957, 0.        , ..., 0.        , 0.        ,\n",
       "        0.01086957],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.01086957,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rubber-advisory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mat_K >0.9).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unsigned-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLR(estimator): \n",
    "    def __init__(self , Kernel = GaussianKernel, \n",
    "                 lam = 1e-9 , sig2 = 1): \n",
    "        super().__init__(Kernel, lam, sig2)\n",
    "        \n",
    "    def fit(self,X,y,max_iter = 10): \n",
    "        self.X_train = X\n",
    "        if self.Kernel == GaussianKernel: \n",
    "            self.mat_K = to_mat_K(X,self.Kernel,self.sig2)\n",
    "        self.alpha = fit_KLR_IRLS(self.mat_K, self.lam, y,max_iter)\n",
    "        self.f = f_from_alpha(self.alpha,self.Kernel,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "continental-finger",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alpha_KLR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-25a3055395f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WKRR :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluate_MSE_from_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_KLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'alpha_KLR' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_MSE_from_alpha(alpha,X,y,lam,mat_K, Kernel = GaussianKernel):\n",
    "    '''\n",
    "    Function that computes the MSE of the vector computed alpha. \n",
    "    \n",
    "    args : \n",
    "            alpha : this is the final value we compute. We do not look directly for a function but for some \n",
    "            parameter that will completely determined the function. alpha is this parameter\n",
    "            X : training data \n",
    "            y : target data \n",
    "            lam : regularization factor\n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            Kernel : the kernel we are using. Normally, mat_K has been computed with the kernel K\n",
    "            \n",
    "    returns : \n",
    "            the MSE of the data plus the regularization factor\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    f_alpha = f_from_alpha(alpha,Kernel,X)\n",
    "    loss = 0\n",
    "    for i in range(n): \n",
    "        loss+= (y[i]-f_alpha(X[i,:]))**2.0\n",
    "    loss/= n\n",
    "    print(' loss without regularization : ', np.round(loss,4)) \n",
    "    reg = lam*alpha@mat_K@alpha\n",
    "    print('regularization :', np.round(reg,4))\n",
    "    return loss + reg \n",
    "\n",
    "\n",
    "print('WKRR :',evaluate_MSE_from_alpha(alpha_KLR, X_train, y_train, lam, mat_K))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
