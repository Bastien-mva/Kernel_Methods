{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy.linalg \n",
    "from tqdm import tqdm\n",
    "import numba \n",
    "from numba import njit,vectorize, jit\n",
    "import time \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/Xtr0_mat100.csv', header=None, delimiter = ' ').to_numpy()\n",
    "y_train = pd.read_csv('data/Ytr0.csv')['Bound'].to_numpy()\n",
    "y_train = 2*y_train-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def GaussianKernel(x,y,sig2 = 1): \n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_for_data(alpha,mat_K,j): \n",
    "    return np.sum([alpha[i]*mat_K[i,j] for i in range(mat_K.shape[0])])\n",
    "\n",
    "def f_from_alpha(alpha, Kernel, X):\n",
    "    '''\n",
    "    Calcule f à partir d'alpha. On utilise ici la forumule du representer thm : \n",
    "    f(x) = sum(alpha_i*K(x_i,x))\n",
    "    \n",
    "    args : \n",
    "        alpha : vecteur de taille (nombre de données dans le dataset). \n",
    "        Kernel : n'importe quel kernel \n",
    "        X : Matrice contenant les données. X.shape[0] doit etre eégal à la taille de alpha\n",
    "        \n",
    "    return : la fonction donnée par le representer theorem\n",
    "    '''\n",
    "    return  lambda x : np.sum([alpha[i]*(Kernel(X[i,:],x)) for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def to_mat_K(X, Kernel, sig2 = 1): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = X[i,:]\n",
    "        for j in range(i,length): \n",
    "            x_j = X[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0\n",
    "sig2 = 1\n",
    "alpha = np.ones(X_train.shape[0])\n",
    "mat_K = to_mat_K(X_train,GaussianKernel, 1)\n",
    "alpha_init = np.ones(mat_K.shape[0])/mat_K.shape[0]\n",
    "vect_W_init = np.ones(mat_K.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(K): \n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente quelques fonctions. Ici, on fait simplement une descente de gradient pour commencer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 20/20 [00:00<00:00, 31.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : 0.004  y : 0\n",
      "1  : 0.006  y : 1\n",
      "2  : 0.005  y : 1\n",
      "3  : 0.008  y : 1\n",
      "4  : 0.004  y : 1\n",
      "5  : -0.001  y : 0\n",
      "6  : 0.003  y : 0\n",
      "7  : 0.006  y : 0\n",
      "8  : 0.007  y : 0\n",
      "9  : -0.002  y : 1\n"
     ]
    }
   ],
   "source": [
    "#@vectorize\n",
    "def loss(u): \n",
    "    return np.log(1+np.exp(-u))\n",
    "def sigmoid(u): \n",
    "    return 1/(1+np.exp(-u))\n",
    "\n",
    "\n",
    "def grad_loss(u): \n",
    "    return -sigmoid(-u)\n",
    "\n",
    "def hess_loss(u): \n",
    "    return sigmoid(u)*sigmoid(-u)\n",
    "\n",
    "def J(alpha, y = y_train, mat_K = mat_K, lam = lam):\n",
    "    n = alpha.shape[0]\n",
    "    regularizer = lam/2*alpha@mat_K@alpha\n",
    "    vect = mat_K@alpha\n",
    "    somme = 1/n*np.sum(loss(y*vect))\n",
    "    return somme+regularizer\n",
    "   \n",
    "def grad_J(alpha, y = y_train, mat_K = mat_K, lam = lam): \n",
    "    n = y.shape[0]\n",
    "    vect_P_alpha = grad_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K@(vect_P_alpha*y)+ lam*mat_K@alpha\n",
    "\n",
    "def hess_J(alpha, y = y_train, mat_K = mat_K, lam = lam):\n",
    "    n = mat_K.shape[0]\n",
    "    vect_W = hess_loss(y*(mat_K@alpha))\n",
    "    return 1/n*mat_K +lam*mat_K\n",
    "\n",
    "def Kernel_logistic_reg_fit(X= X_train, y = y_train, mat_K = mat_K, lam = lam, Niter =20):\n",
    "    alpha = 0.00000*np.random.randn(X.shape[0])\n",
    "    #alpha = np.ones(2000)\n",
    "    mat_K = standardize(mat_K)\n",
    "    lr = 5\n",
    "    for i in tqdm(range(Niter)): \n",
    "        #inv = np.linalg.inv(hess_J(alpha, mat_K = mat_K))\n",
    "        #alpha-= lr*inv@grad_J(alpha ,mat_K = mat_K)#, mat_K= mat_K)\n",
    "        alpha-= lr*grad_J(alpha ,mat_K = mat_K)\n",
    "        '''\n",
    "        if i%1 ==0 : \n",
    "            print('alpha :', alpha)\n",
    "            print('J :',J(alpha,mat_K = mat_K))\n",
    "            print('grad :', grad_J(alpha,mat_K = mat_K))\n",
    "    print('alpha_end :', alpha)\n",
    "    print('J_end :',J(alpha,mat_K = mat_K))\n",
    "    print('grad_end :', grad_J(alpha,mat_K = mat_K))'''\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : 0.004  y : 0\n",
      "1  : 0.006  y : 1\n",
      "2  : 0.005  y : 1\n",
      "3  : 0.008  y : 1\n",
      "4  : 0.004  y : 1\n",
      "5  : -0.001  y : 0\n",
      "6  : 0.003  y : 0\n",
      "7  : 0.006  y : 0\n",
      "8  : 0.007  y : 0\n",
      "9  : -0.002  y : 1\n"
     ]
    }
   ],
   "source": [
    "f = f_from_alpha(first_alpha, GaussianKernel, X_train)\n",
    "for i in range(10): \n",
    "        print(i ,' :',np.round(f(X_train[i,:]),3), ' y :', y_train[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1e-8 marche bien pour lambda\n",
    "def fit_KRR(mat_K,lam,y):\n",
    "    #mat_K = standardize(mat_K) #marche pas si on standardise \n",
    "    n = mat_K.shape[0]\n",
    "    full_mat = mat_K +n*lam*np.eye(n)\n",
    "    alpha = np.linalg.solve(full_mat,y)\n",
    "    return alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : -1.0  y : -1\n",
      "1  : 1.0  y : 1\n",
      "2  : 1.0  y : 1\n",
      "3  : 1.0  y : 1\n",
      "4  : 1.0  y : 1\n",
      "5  : -1.0  y : -1\n",
      "6  : -1.0  y : -1\n",
      "7  : -1.0  y : -1\n",
      "8  : -1.0  y : -1\n",
      "9  : 1.0  y : 1\n",
      "diff : 1360660.471641838\n"
     ]
    }
   ],
   "source": [
    "# meme fonction que WKRR, mais on est environ 10 fois plus lent\n",
    "'''\n",
    "def fit_WKRR(mat_K,vect_W, lam, y): \n",
    "    # pour l'instant on suppose que W est bien inversible, i.e. aucune valeur à zéro\n",
    "    n = mat_K.shape[0]\n",
    "    mat_sqrt_W = np.diag(np.sqrt(vect_W))\n",
    "    mat_neg_sqrt_W = np.diag(1/np.sqrt(vect_W))\n",
    "    big_mat = mat_sqrt_W@mat_K@mat_sqrt_W + n*lam*np.eye(n)\n",
    "    return scipy.linalg.solve(big_mat@mat_neg_sqrt_W,mat_sqrt_W@y)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_WKRR(mat_K,vect_W,lam,y): \n",
    "    '''\n",
    "    Compute the Weighted Kernel Redge Regression. the Formula is given in the course. \n",
    "    The code is optimized, we do not take the diagonal matrix of the square root of W. Instead, \n",
    "    we only compute some np.multiply stuff. \n",
    "    \n",
    "    args : \n",
    "    \n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            vect_W : the vector that contains the weight associated to each sample. here we need that all the \n",
    "            coefficient of this vector is 0. Otherwise we won't be able to compute the inverse of the square root\n",
    "            lam : regularization factor \n",
    "            y : the vector we train on \n",
    "    \n",
    "    returns :\n",
    "            \n",
    "            the vector alpha that satisfy the formula in the course. \n",
    "    alpha then needs to be transformed to a function in order to fit the data.\n",
    "    '''\n",
    "    min_W = np.min(vect_W)\n",
    "    if (min_W < 0) or (min_W == 0) : \n",
    "        print('Non invertible Matrix W ')\n",
    "    n = mat_K.shape[0]\n",
    "    vect_sqrt_W = np.sqrt(vect_W) # the square root of the original vector\n",
    "    vect_neg_sqrt_W = 1/vect_sqrt_W # the negative square root of the original vector\n",
    "    b = np.multiply(vect_sqrt_W,y) \n",
    "    big_mat = np.multiply(np.multiply(vect_sqrt_W.reshape(-1,1),mat_K), vect_sqrt_W) +n*lam*np.eye(n)\n",
    "    A = np.multiply(vect_neg_sqrt_W,big_mat)\n",
    "    return scipy.linalg.solve(A,b)\n",
    "\n",
    "vect_W_init = np.full(mat_K.shape[0],1)#/mat_K.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IRLS(K, y, alpha):\n",
    "        \"\"\"\n",
    "        Iterative step to update alpha when training the classifier\n",
    "        :param K: np.array, kernel\n",
    "        :param y: np.array, labels\n",
    "        :param alpha: np.array\n",
    "        :return: - W: np.array\n",
    "                 - z: np.array\n",
    "        \"\"\"\n",
    "        m = np.dot(K, alpha)\n",
    "        W = sigmoid(m) * sigmoid(-m)\n",
    "        z = m + y/sigmoid(-y*m)\n",
    "        return W, z\n",
    "\n",
    "def WKRR_af(K, W, z):\n",
    "        \"\"\"\n",
    "        Compute new alpha\n",
    "        :param K: np.array, kernel\n",
    "        :param W: np.array\n",
    "        :param z: np.array\n",
    "        :return: np.array, new alpha\n",
    "        \"\"\"\n",
    "        n = K.shape[0]\n",
    "        W_s = np.diag(np.sqrt(W))\n",
    "        A = np.dot(np.dot(W_s, K), W_s) + n * lam * np.eye(n)\n",
    "        A = np.dot(np.dot(W_s, np.linalg.inv(A)), W_s)\n",
    "        return np.dot(A, z)\n",
    "    \n",
    "    \n",
    "def recoding_KRL(mat_K,lam,y, max_iter = 10): \n",
    "    n = mat_K.shape[0]\n",
    "    old_alpha = 0*np.ones(n)\n",
    "    for i in range(max_iter): \n",
    "        W,z = IRLS(mat_K,y,old_alpha)\n",
    "        alpha = fit_WKRR(mat_K, W, lam, z)\n",
    "        f = f_from_alpha(alpha, GaussianKernel, X_train)\n",
    "        old_alpha = np.copy(alpha)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_m(mat_K,alpha): \n",
    "    return mat_K@alpha\n",
    "\n",
    "def compute_P(y,m): \n",
    "    return -sigmoid(-np.multiply(y,m))\n",
    "\n",
    "def compute_W(m):\n",
    "    return np.multiply(sigmoid(m),sigmoid(m))\n",
    "\n",
    "def compute_z(y,m): \n",
    "    return m + np.multiply(y,1/sigmoid(-np.multiply(y,m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : -2.0  y : -1\n",
      "1  : 2.0  y : 1\n",
      "2  : 2.0  y : 1\n",
      "3  : 2.0  y : 1\n",
      "4  : 2.0  y : 1\n",
      "5  : -2.0  y : -1\n",
      "6  : -2.0  y : -1\n",
      "7  : -2.0  y : -1\n",
      "8  : -2.0  y : -1\n",
      "9  : 2.0  y : 1\n",
      "0  : -10.389  y : -1\n",
      "1  : 10.389  y : 1\n",
      "2  : 10.389  y : 1\n",
      "3  : 10.389  y : 1\n",
      "4  : 10.389  y : 1\n",
      "5  : -10.389  y : -1\n",
      "6  : -10.389  y : -1\n",
      "7  : -10.389  y : -1\n",
      "8  : -10.389  y : -1\n",
      "9  : 10.389  y : 1\n",
      "0  : -32513.366  y : -1\n",
      "1  : 32513.363  y : 1\n",
      "2  : 32513.358  y : 1\n",
      "3  : 32513.361  y : 1\n",
      "4  : 32513.36  y : 1\n",
      "5  : -32513.365  y : -1\n",
      "6  : -32513.364  y : -1\n",
      "7  : -32513.364  y : -1\n",
      "8  : -32513.366  y : -1\n",
      "9  : 32513.359  y : 1\n",
      "Non invertible Matrix W \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "In  \u001b[0;34m[241]\u001b[0m:\nLine \u001b[0;34m34\u001b[0m:    \u001b[37m#def fit_KLR_IRLS(mat_K, lam, y, max_iter = 10):\u001b[39;49;00m\n",
      "In  \u001b[0;34m[241]\u001b[0m:\nLine \u001b[0;34m22\u001b[0m:    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(max_iter):\n",
      "In  \u001b[0;34m[234]\u001b[0m:\nLine \u001b[0;34m29\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m scipy.linalg.solve(A,b)\n",
      "File \u001b[0;34m/home/bastien/anaconda3/envs/ml/lib/python3.8/site-packages/scipy/linalg/basic.py\u001b[0m, in \u001b[0;32msolve\u001b[0m:\nLine \u001b[0;34m137\u001b[0m:   a1 = atleast_2d(_asarray_validated(a, check_finite=check_finite))\n",
      "File \u001b[0;34m/home/bastien/anaconda3/envs/ml/lib/python3.8/site-packages/scipy/_lib/_util.py\u001b[0m, in \u001b[0;32m_asarray_validated\u001b[0m:\nLine \u001b[0;34m263\u001b[0m:   a = toarray(a)\n",
      "File \u001b[0;34m/home/bastien/.local/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m, in \u001b[0;32masarray_chkfinite\u001b[0m:\nLine \u001b[0;34m485\u001b[0m:   \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_KLR_IRLS(mat_K, lam, y, max_iter = 10): \n",
    "    '''\n",
    "    Fonction qui optimise la loss définie par la la Kernel Logistic Regression. \n",
    "    \n",
    "    args : \n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            \n",
    "            lam : regularization factor \n",
    "            \n",
    "            y : the vector we train on. Must be -1 or 1 \n",
    "            \n",
    "            max_iter : the maximum number of iteration we are ready to do \n",
    "    returns : \n",
    "            the vector alpha optimized \n",
    "            alpha then needs to be transformed to a function in order to fit the data.\n",
    "    '''\n",
    "    alpha = np.zeros(mat_K.shape[0])\n",
    "    m = compute_m(mat_K,alpha)    \n",
    "    W = compute_W(m)\n",
    "    z = compute_z(y,m)\n",
    "    for i in range(max_iter): \n",
    "        alpha = fit_WKRR(mat_K,W,lam,z)\n",
    "        m = compute_m(mat_K,alpha)\n",
    "        W = compute_W(m)\n",
    "        z = compute_z(y,m)\n",
    "        f = f_from_alpha(alpha, GaussianKernel, X_train)\n",
    "        for i in range(10): \n",
    "            print(i ,' :',np.round(f(X_train[i,:]),3), ' y :', y_train[i])\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "#def fit_KLR_IRLS(mat_K, lam, y, max_iter = 10):\n",
    "alpha_KLR = fit_KLR_IRLS(mat_K, lam,y_train)  \n",
    "f_KLR = f_from_alpha(alpha_KLR, GaussianKernel, X_train)\n",
    "for i in range(10): \n",
    "        print(i ,' :',np.round(f_KLR(X_train[i,:]),3), ' y :', y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator(): \n",
    "    def __init__(self , Kernel = GaussianKernel, lam = 1e-8, sig2 = 1 ): \n",
    "        self.Kernel = Kernel\n",
    "        self.lam = lam \n",
    "        self.sig2 = sig2 \n",
    "        self.mat_K = None \n",
    "        self.alpha = None \n",
    "        self.f = None \n",
    "        \n",
    "    def predict_proba(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            probs = np.empty(X.shape[0])\n",
    "            for i in range(X.shape[0]): \n",
    "                probs[i] = self.f(X[i,:])\n",
    "            return probs \n",
    "    \n",
    "    def predict(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(X)\n",
    "            return prob>0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lam = 1e-8 est bien\n",
    "class KRR(): \n",
    "    def __init__(self , Kernel = GaussianKernel, lam = 1e-8, sig2 = 1 ): \n",
    "        self.Kernel = Kernel\n",
    "        self.lam = lam \n",
    "        self.sig2 = sig2 \n",
    "        self.mat_K = None \n",
    "        self.alpha = None \n",
    "        self.f = None \n",
    "        #self.vect_W = vect_W \n",
    "    def fit(self, X, y): \n",
    "        if self.Kernel == GaussianKernel : \n",
    "            self.mat_K = to_mat_K(X, self.Kernel,self.sig2)\n",
    "        self.alpha = fit_KRR(self.mat_K, self.lam, y)\n",
    "        self.f = f_from_alpha(self.alpha,self.Kernel,X)\n",
    "    \n",
    "    def predict_proba(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            probs = np.empty(X.shape[0])\n",
    "            for i in range(X.shape[0]): \n",
    "                probs[i] = self.f(X[i,:])\n",
    "            return probs \n",
    "    def predict(self,X): \n",
    "        if self.f == None : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(X)\n",
    "            return prob>0.5\n",
    "        \n",
    "# test = KRR(sig2=1)\n",
    "# test.fit(X_train,y_train)\n",
    "# np.sum(np.abs(test.predict(X_train)-y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLR(estimator): \n",
    "    def __init__(self , Kernel = GaussianKernel, \n",
    "                 lam = 1e-9 , sig2 = 1): \n",
    "        super().__init__(Kernel, lam, sig2)\n",
    "        \n",
    "    def fit(self,X,y,max_iter = 10): \n",
    "        if self.Kernel == GaussianKernel: \n",
    "            self.mat_K = to_mat_K(X,self.Kernel,self.sig2)\n",
    "        self.alpha = fit_KLR_IRLS(self.mat_K, self.lam, y,max_iter)\n",
    "        self.f = f_from_alpha(self.alpha,self.Kernel,X)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss without regularization :  0.0\n",
      "regularization : 0.0\n",
      "WKRR : 3.078269688032865e-17\n"
     ]
    }
   ],
   "source": [
    "def evaluate_MSE_from_alpha(alpha,X,y,lam,mat_K, Kernel = GaussianKernel):\n",
    "    '''\n",
    "    Function that computes the MSE of the vector computed alpha. \n",
    "    \n",
    "    args : \n",
    "            alpha : this is the final value we compute. We do not look directly for a function but for some \n",
    "            parameter that will completely determined the function. alpha is this parameter\n",
    "            X : training data \n",
    "            y : target data \n",
    "            lam : regularization factor\n",
    "            mat_K : Kernel Matrix that contains the information in the data (K_ij=K(x_i,x_j))\n",
    "            Kernel : the kernel we are using. Normally, mat_K has been computed with the kernel K\n",
    "            \n",
    "    returns : \n",
    "            the MSE of the data plus the regularization factor\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    f_alpha = f_from_alpha(alpha,Kernel,X)\n",
    "    loss = 0\n",
    "    for i in range(n): \n",
    "        loss+= (y[i]-f_alpha(X[i,:]))**2.0\n",
    "    loss/= n\n",
    "    print(' loss without regularization : ', np.round(loss,4)) \n",
    "    reg = lam*alpha@mat_K@alpha\n",
    "    print('regularization :', np.round(reg,4))\n",
    "    return loss + reg \n",
    "\n",
    "\n",
    "print('WKRR :',evaluate_MSE_from_alpha(alpha_KLR, X_train, y_train, lam, mat_K))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
