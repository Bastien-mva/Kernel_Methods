{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thorough-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We do a little grid search to show that it works well\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:14,  2.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7375\n",
      "We have tested  :  0.00788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:13,  2.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7315\n",
      "We have tested  :  0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:14,  2.97s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.732\n",
      "We have tested  :  0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:14,  2.99s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.727\n",
      "We have tested  :  0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:17,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.738\n",
      "We have tested  :  0.0086\n",
      "###########BEST_PARAM =  0.0086 with score : 0.738\n",
      "Model 0 Predicted\n",
      "Model 1 Predicted\n",
      "Model 2 Predicted\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg \n",
    "from tqdm import tqdm\n",
    "import numba\n",
    "from numba import njit,vectorize, jit\n",
    "from itertools import product, permutations\n",
    "import time\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "\n",
    "import Estimators\n",
    "import Unsupervised\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def importation_mat_100(): \n",
    "    '''\n",
    "    import all the (float) data and puts them in a list X_train_100. X_train_100[0] will be X_train_mat_100 and so on. \n",
    "    '''\n",
    "    X_train_100 = []\n",
    "    X_test_100 = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtrain = np.squeeze(xtrain.to_numpy())\n",
    "        X_train_100.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtest = np.squeeze(xtest.to_numpy())\n",
    "        X_test_100.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train_100,X_test_100,Y_train\n",
    "\n",
    "def importation(): \n",
    "    '''\n",
    "    import all the (string) data and puts them in a list X_train. X_train[0] will be X_train_0 and so on. \n",
    "    '''\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtrain = xtrain.iloc[1:,1].to_numpy()\n",
    "        X_train.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtest = xtest.iloc[1:,1].to_numpy()\n",
    "        X_test.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train,X_test,Y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@njit\n",
    "def PolynomialKernel(x,y): \n",
    "    return (x.dot(y)+c)**d\n",
    "\n",
    "@njit\n",
    "def GaussianKernel(x,y,sig2 = 1):\n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))\n",
    "\n",
    "@njit\n",
    "def LinearKernel(x,y): \n",
    "    return x.dot(y)\n",
    "\n",
    "@njit\n",
    "def Laplace_kernel(x, y, gamma=1):\n",
    "    return 0.5 * np.exp(-gamma * np.linalg.norm(x-y))\n",
    "\n",
    "\n",
    "def to_Kernel_train(X, Kernel): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in tqdm(range(length)):\n",
    "        x_i = np.squeeze(X[i])\n",
    "        for j in range(i,length): \n",
    "            x_j = np.squeeze(X[j])\n",
    "            value = Kernel(x_i,x_j)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "#@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel):\n",
    "    '''\n",
    "    takes the training data input Xtrain and the test data Xtest and computes the Kernel_test. \n",
    "    \n",
    "    The length of the resulting Kernel_test will be (nb_traing_samples, nb_testing_samples) \n",
    "    '''\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in tqdm(range(length_train)):\n",
    "        x_i = np.squeeze(Xtrain[i])\n",
    "        for j in range(length_test): \n",
    "            x_j = np.squeeze(Xtest[j])\n",
    "            bimat_K[i,j] = Kernel(x_i,x_j)\n",
    "    return bimat_K\n",
    "\n",
    "def standardize(K): \n",
    "    '''\n",
    "    standardize the given matrix K\n",
    "    '''\n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)\n",
    "\n",
    "# Compute phi for Spectrum kernel\n",
    "def phi_spectrum(x,k,U):\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi_spec = np.zeros(len(U))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in range(len(x)-k+1):\n",
    "            if x[i:i+k] == u:\n",
    "                phi_spec[j] +=1\n",
    "    return phi_spec\n",
    "\n",
    "#Compute phi for Substring kernel\n",
    "def l(i):\n",
    "    return i[-1] - i[0] + 1\n",
    "\n",
    "def I(k,n):\n",
    "    I = set()\n",
    "    for ele in permutations(range(0,n),k):\n",
    "        I.add(tuple(sorted(list(ele))))\n",
    "    return list(I)\n",
    "\n",
    "def phi_substring(x,k,U,lamb=0.5): #fonctionne (test√© avec l'exemple page 392)\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi = np.zeros(len(U))\n",
    "    I_kn = I(k,len(x))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in I_kn:\n",
    "            x_i = \"\".join([x[idx]  for idx in i])\n",
    "            if x_i==u:\n",
    "                phi[j] += lamb**l(i)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def make_dict_phi(X, phi,k):\n",
    "    U = [''.join(letter) for letter in product('ACGT', repeat=k)]\n",
    "    phi_dict = {seq:phi(seq,k,U) for seq in tqdm(X)}\n",
    "    return phi_dict\n",
    "\n",
    "\n",
    "## just uncomment the ones you want to compute. (and uncomment the next line also). for k=8, takes about 2 hours. \n",
    "dict_spectrum_traintest = [0]*10\n",
    "\n",
    "#dict_spectrum_traintest[3] = make_dict_phi(X_traintest[0],phi_spectrum,3)\n",
    "#dict_spectrum_traintest[4] = make_dict_phi(X_traintest[0],phi_spectrum,4)\n",
    "\n",
    "#dict_spectrum_traintest[5] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,5)\n",
    "#dict_spectrum_traintest[6] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,6)\n",
    "#dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)\n",
    "#dict_spectrum_traintest[8] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,8)\n",
    "#dict_sub_traintest_2 = make_dict_phi(X_traintest[0], phi_substring,2)\n",
    "\n",
    "def make_K_spectrum(k): \n",
    "    '''\n",
    "    template function to build a K_spectrum_function. \n",
    "    We need to have one function for each kernel functionin order to compute efficiently the Kernel matrix (it makes code fluent). \n",
    "    '''\n",
    "    def K_spectrum(x,y): \n",
    "        value = np.sum([dict_spectrum_traintest[k][str(x)] * dict_spectrum_traintest[k][str(y)]])\n",
    "        return value \n",
    "    return K_spectrum\n",
    "\n",
    "K_spectrum_5 = make_K_spectrum(5)\n",
    "K_spectrum_6 = make_K_spectrum(6)\n",
    "K_spectrum_7 = make_K_spectrum(7)\n",
    "K_spectrum_8 = make_K_spectrum(8)\n",
    "\n",
    "\n",
    "def get_first_Kernel(Kernel): \n",
    "    '''\n",
    "    gets the first Kernel_train and Kernel_test. This is only for testing one model one the first dataset. \n",
    "    '''\n",
    "    K_train = to_Kernel_train(X_train[0], Kernel)\n",
    "    K_test = to_Kernel_test(X_train[0], X_test[0], Kernel)\n",
    "    return K_train,K_test \n",
    "\n",
    "def get_list_Kernels_100(Kernel) : \n",
    "    '''\n",
    "    get two list ok Kernel. For list_K_train, each element i is the Kernel_train computed with kernel for dataset i. Same for list_K_test\n",
    "    args : \n",
    "        \n",
    "        Kernel : a Kernel function, for example K_sprectum_6\n",
    "        \n",
    "    returns : \n",
    "    \n",
    "            List_Kernel_train : a list where each element i is the Kernel_train computed with Kernel for dataset i.\n",
    "            \n",
    "            List_Kernel_test : a list where each element i is the Kernel_test computed with Kernel for dataset i. \n",
    "            \n",
    "    Only works for Kernel that takes as input floats, for example GaussianKernel\n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train_100[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train_100[i],X_test_100[i], Kernel))\n",
    "    return list_K_train,list_K_test\n",
    "\n",
    "def get_list_Kernels(Kernel) : \n",
    "    '''\n",
    "    We will mostly use this function. \n",
    "    get two list ok Kernel. For list_K_train, each element i is the Kernel_train computed with kernel for dataset i. Same for list_K_test\n",
    "    args : \n",
    "        \n",
    "        Kernel : a Kernel function, for example K_sprectum_6\n",
    "        \n",
    "    returns : \n",
    "    \n",
    "            List_Kernel_train : a list where each element i is the Kernel_train computed with Kernel for dataset i.\n",
    "            \n",
    "            List_Kernel_test : a list where each element i is the Kernel_test computed with Kernel for dataset i. \n",
    "            \n",
    "    Only works for Kernel that takes as input strings, for example K_spectrum. \n",
    "    \n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train[i],X_test[i], Kernel))\n",
    "    return list_K_train,list_K_test\n",
    "\n",
    "\n",
    "def grid_search_cv(model, Kernel_train, Y_train, parameters): \n",
    "    '''\n",
    "    grid search cv. \n",
    "    put the model you want, for example Estimators.KRR(), the Kernel_train you want to train on, the labels Y_train\n",
    "    and the list of parameters (parameters) you want to try. \n",
    "    \n",
    "    Will print the best parameter with the score associated. \n",
    "    '''\n",
    "    scores = list()\n",
    "    for parameter in parameters : \n",
    "        model.set_parameter(parameter)\n",
    "        scores.append(model.cross_val(Kernel_train,Y_train, 5))\n",
    "        print('We have tested  : ', np.round(parameter, 6))\n",
    "    arg_max = np.argmax(np.array(scores))\n",
    "    print('###########BEST_PARAM = ',np.round(parameters[arg_max], 6), 'with score :', scores[arg_max])\n",
    "    return parameters[arg_max]\n",
    "\n",
    "\n",
    "def download_results(model,list_Kernel_train,list_Kernel_test, parameters, name_dossier):\n",
    "    '''\n",
    "    Download the predictions of the 3 datasets, where the model will be trained with the Kernel_train you give and the parameter you give. \n",
    "    \n",
    "    args : \n",
    "            model : a model for example Estimators.KRR()\n",
    "            list_Kernel_train : a list where the element i is the Kernel_train of the i dataset (not any Kernel_train, the one computed with the Kernel of your choice)\n",
    "            list_Kernel_test : a list where the element i is the Kernel_test of the i dataset (not any Kernel_test, the one computed with the Kernel of your choice)\n",
    "            parameters : a list of parameters you want to try. \n",
    "            name_dossier : your predictions will be stocked in the dossier 'predictions_KM'+name_dossier+'.csv'\n",
    "            \n",
    "    returns : None. \n",
    "    \n",
    "    All your predictions will be directly saved. \n",
    "    '''\n",
    "    \n",
    "    Y_predicted = []\n",
    "    for i in range(3): \n",
    "        model.set_parameter(parameters[i])\n",
    "        model.fit(list_Kernel_train[i], Y_train[i])\n",
    "        Y_predicted.append(model.predict(list_Kernel_test[i])*1)\n",
    "        print('Model {} Predicted'.format(i))\n",
    "\n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('Results/predictions_KM'+name_dossier+'.csv', index=False)\n",
    "    \n",
    "    \n",
    "def downloads_from_voting(models, lists_Kernel_train,lists_Kernel_test,parameterss, name_dossier): \n",
    "    '''\n",
    "    Do the same as download_result but takes more models and make a prediction based on the prediction of all your models. \n",
    "    \n",
    "    \n",
    "    Takes some models and computes the results by a voting process. \n",
    "    args : \n",
    "            lists_Kernel_train : list of list_Kernel_train.\n",
    "            \n",
    "            lists_Kernel_test : list of list_Kernel_test. \n",
    "\n",
    "            parameterss :list of list of parameter (the best parameter possible). each list of parameters contains 3 parameter : one for each dataset. \n",
    "            \n",
    "            name_dossier :  your predictions will be stocked in the dossier 'predictions_KM'+name_dossier+'.csv'. should be informative\n",
    "            \n",
    "            \n",
    "    returns : None\n",
    "    It automatically downloads the predictions. \n",
    "    \n",
    "    Note that here, if the models disagree and have 50% yes 50% no, then it will be the first model of the list\n",
    "    that will have the last word (works fine for 2 models but not tested for more)\n",
    "    \n",
    "    Maybe we should print the correlation between the models. (To add)\n",
    "    '''\n",
    "    Y_predicted = []\n",
    "    length = len(models)\n",
    "    for i in range(3): \n",
    "        y_pred = list()\n",
    "        for model,list_Kernel_train,list_Kernel_test,parameters in zip(models, lists_Kernel_train, lists_Kernel_test, parameterss): \n",
    "            model.set_parameter(parameters[i])\n",
    "            model.fit(list_Kernel_train[i], Y_train[i])\n",
    "            y_pred.append(model.predict(list_Kernel_test[i]).reshape(-1,1))\n",
    "        y_pred = np.array(np.concatenate(y_pred, axis = 1),float)\n",
    "        y_pred[:,0] = y_pred[:,0]*2-0.5\n",
    "        Y_predicted.append((np.sum(y_pred,axis = 1)>0.5)*1)\n",
    "        \n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM'+name_dossier+'.csv', index=False)\n",
    "            \n",
    "\n",
    "        \n",
    "#if __name__ == __main__: \n",
    "\n",
    "X_train,X_test,Y_train = importation()\n",
    "X_train_100, X_test_100,Y_train = importation_mat_100()\n",
    "'''\n",
    "we do this so that we compute the Dictionnary of phi_spectrum easier. \n",
    "'''\n",
    "X_traintest = []\n",
    "for i in range(3): \n",
    "    X_traintest.append(np.concatenate((X_train[i],X_test[i]))) \n",
    "        \n",
    "# We load the Kernel matrices we have already computed. It can be recomputed but take some time. \n",
    "# if needed, you only have to uncomment the next : \n",
    "'''\n",
    "K_spectrum_5 = make_K_spectrum(5)\n",
    "K_spectrum_6 = make_K_spectrum(6)\n",
    "K_spectrum_7 = make_K_spectrum(7)\n",
    "K_spectrum_8 = make_K_spectrum(8)\n",
    "dict_spectrum_traintest = [0]*10\n",
    "dict_spectrum_traintest[3] = make_dict_phi(X_traintest[0],phi_spectrum,3)\n",
    "dict_spectrum_traintest[4] = make_dict_phi(X_traintest[0],phi_spectrum,4)\n",
    "\n",
    "dict_spectrum_traintest[5] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,5)\n",
    "dict_spectrum_traintest[6] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,6)\n",
    "dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)\n",
    "dict_spectrum_traintest[8] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,8)\n",
    "\n",
    "list_Kernel_train_spectrum_5, list_Kernel_test_spectrum_5 = get_list_Kernels(K_spectrum_5)  \n",
    "list_Kernel_train_spectrum_6, list_Kernel_test_sprectum_6 = get_list_Kernels(K_spectrum_6)\n",
    "list_Kernel_train_spectrum_7, list_Kernel_test_spectrum_7 = get_list_Kernels(K_spectrum_7)  \n",
    "list_Kernel_train_spectrum_8, list_Kernel_test_spectrum_8 = get_list_Kernels(K_spectrum_8) \n",
    "'''\n",
    "\n",
    "list_Kernel_train_spectrum_6 = load('PrecomputeKernels/list_Kernel_train_spectrum_6.npy')\n",
    "list_Kernel_train_spectrum_7 = load('PrecomputeKernels/list_Kernel_train_spectrum_7.npy')\n",
    "list_Kernel_train_spectrum_8 = load('PrecomputeKernels/list_Kernel_train_spectrum_8.npy')\n",
    "\n",
    "list_Kernel_test_spectrum_6 = load('PrecomputeKernels/list_Kernel_test_spectrum_6.npy')\n",
    "list_Kernel_test_spectrum_7 = load('PrecomputeKernels/list_Kernel_test_spectrum_7.npy')\n",
    "list_Kernel_test_spectrum_8 = load('PrecomputeKernels/list_Kernel_test_spectrum_8.npy')\n",
    "\n",
    "\n",
    "KRR = Estimators.KRR()\n",
    "\n",
    "SVM = Estimators.SVM()\n",
    "\n",
    "\n",
    "\n",
    "best_list_Kernel_train = [list_Kernel_train_spectrum_8[0], list_Kernel_train_spectrum_6[1], list_Kernel_train_spectrum_7[2]]\n",
    "best_list_Kernel_test = [list_Kernel_test_spectrum_8[0], list_Kernel_test_spectrum_6[1], list_Kernel_test_spectrum_7[2]]\n",
    "best_parameters = [0.007091,  0.001728, 0.00788 ]\n",
    "\n",
    "print('We do a little grid search on the last dataset with SVM to show that it works well')\n",
    "grid_search_cv(SVM, best_list_Kernel_train[2], Y_train[2], [0.00788,0.0099, 0.0056, 0.0048, 0.0086])\n",
    "\n",
    "download_results(SVM,best_list_Kernel_train,best_list_Kernel_test, best_parameters, '_Final')\n",
    "\n",
    "\n",
    "sig2 = 1\n",
    "d = 1.1\n",
    "c = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
