{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qpsolvers\n",
      "  Downloading qpsolvers-1.5.tar.gz (10 kB)\n",
      "Collecting quadprog>=0.1.8\n",
      "  Downloading quadprog-0.1.8.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 619 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Cython\n",
      "  Using cached Cython-0.29.22-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Building wheels for collected packages: qpsolvers, quadprog\n",
      "  Building wheel for qpsolvers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for qpsolvers: filename=qpsolvers-1.5-py3-none-any.whl size=18784 sha256=573ab61b01bf5d4cf3fcc394637ea2e18f27606bb498dfc2400ff4f494b65272\n",
      "  Stored in directory: /home/bastien/.cache/pip/wheels/f1/ec/63/6cf0699e79ab55a076e8102b075cb9967eda37dc18ec300c6b\n",
      "  Building wheel for quadprog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for quadprog: filename=quadprog-0.1.8-cp38-cp38-linux_x86_64.whl size=540821 sha256=388d5d1bf306665da24882799edc67b8efef0e03094bc1de01cbc6b1d06eec5a\n",
      "  Stored in directory: /home/bastien/.cache/pip/wheels/b7/9a/ae/8b1455b942197ab2631f74f0e47dafa1cc1c3aeea28d8953c4\n",
      "Successfully built qpsolvers quadprog\n",
      "Installing collected packages: Cython, quadprog, qpsolvers\n",
      "Successfully installed Cython-0.29.22 qpsolvers-1.5 quadprog-0.1.8\n",
      "Collecting cvxopt\n",
      "  Downloading cvxopt-1.2.6-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.2.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install qpsolvers\n",
    "#!pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg\n",
    "from tqdm import tqdm\n",
    "import numba \n",
    "from numba import njit, vectorize, jit\n",
    "import time \n",
    "import scipy\n",
    "\n",
    "import qpsolvers\n",
    "from qpsolvers import solve_qp\n",
    "from qpsolvers import dense_solvers, sparse_solvers\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "\n",
    "import Estimators\n",
    "from Estimators import sigmoid, Kernel_cross_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjacent-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour récupérer toutes les données\n",
    "X_train0 = pd.read_csv('data/Xtr0_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train0 = pd.read_csv('data/Ytr0.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test0 = pd.read_csv('data/Xte0_mat100.csv',delimiter= ' ', header= None).values\n",
    "\n",
    "X_train1 = pd.read_csv('data/Xtr1_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train1 = pd.read_csv('data/Ytr1.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test1 = pd.read_csv('data/Xte1_mat100.csv',delimiter= ' ', header= None).values\n",
    "\n",
    "X_train2 = pd.read_csv('data/Xtr2_mat100.csv',delimiter= ' ', header= None).values\n",
    "Y_train2 = pd.read_csv('data/Ytr2.csv',delimiter= ',')['Bound'].to_numpy()\n",
    "X_test2 = pd.read_csv('data/Xte2_mat100.csv',delimiter= ' ', header= None).values\n",
    "\n",
    "X_train = [X_train0,X_train1,X_train2]\n",
    "X_test = [X_test0,X_test1,X_test2]\n",
    "Y_train = [Y_train0,Y_train1,Y_train2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painful-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def GaussianKernel(x, y, sig2 = 1): \n",
    "    return np.exp(-numpy.linalg.norm(x-y)**2/(2*sig2))\n",
    "@njit\n",
    "def Linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "@njit\n",
    "def Polynomial_kernel(x, y, p=5):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "@njit\n",
    "def Laplace_kernel(x, y, gamma=1):\n",
    "    return 0.5 * np.exp(-gamma * numpy.linalg.norm(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imported-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def to_Kernel_train(X, Kernel, sig2 = 1): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = X[i,:]\n",
    "        for j in range(i,length): \n",
    "            x_j = X[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel,sig2=1):\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in range(length_train):\n",
    "        x_i = Xtrain[i,:]\n",
    "        for j in range(length_test): \n",
    "            x_j = Xtest[j,:]\n",
    "            value = Kernel(x_i,x_j,sig2)\n",
    "            bimat_K[i,j] = value\n",
    "    return bimat_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%time Kernel_train = to_Kernel_train(X_train0,GaussianKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developmental-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_split(Xtrain, ytrain, cv):\n",
    "    idx = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(idx) # we shuffle the indices to get random samples\n",
    "    sample_size = Xtrain.shape[0]//cv\n",
    "    Xtrainsplit = []# a list that wil contain each X_train vector. Each element will be smaller than \n",
    "                    #X_train. If cv = 3 for example, the size (on the x axis) will be 2/3 the original size \n",
    "    ytrainsplit = []\n",
    "    Xvalsplit = []\n",
    "    yvalsplit = []\n",
    "    for i in range(cv-1): \n",
    "        #we add the new indices. Here, takes the original vector and returns the vector without the \n",
    "        # indices passes in argument \n",
    "        Xtrainsplit.append(np.delete(Xtrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        ytrainsplit.append(np.delete(ytrain,idx[i*sample_size:(i+1)*sample_size],axis = 0))\n",
    "        \n",
    "        # we add the rest \n",
    "        # note that here, we keep the same labels for X ( we do not shuffle independantly X and y)\n",
    "        Xvalsplit.append( Xtrain[idx[i*sample_size:(i+1)*sample_size],:])\n",
    "        yvalsplit.append(ytrain[idx[i*sample_size:(i+1)*sample_size]])\n",
    "    # we add the last round. It is different since we can't take float proportion of an array, \n",
    "    # we have to take an integer. So, here we just add what remains. \n",
    "    Xtrainsplit.append(np.delete(Xtrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    ytrainsplit.append(np.delete(ytrain,idx[(cv-1)*sample_size:],axis = 0))\n",
    "    Xvalsplit.append( Xtrain[idx[(cv-1)*sample_size:],:])\n",
    "    yvalsplit.append(ytrain[idx[(cv-1)*sample_size:]])\n",
    "    return Xtrainsplit,Xvalsplit,ytrainsplit,yvalsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "typical-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_results(Kernel , cs,name_dossier):\n",
    "    Y_predicted = []\n",
    "    for i in range(3) : \n",
    "        model = SVM_1(Kernel, C = cs[i])\n",
    "        model.fit(X_train[i], Y_train[i])\n",
    "        Y_predicted.append(model.predict(X_test[i]))\n",
    "        print('Model {} Predicted'.format(i))\n",
    "\n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM'+name_dossier+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "macro-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cest 0\n",
      "cest 1\n",
      "cest 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): \n",
    "    print('cest {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-character",
   "metadata": {},
   "source": [
    "J'ai fais 2 SVM avec deux technique différentes, et ils donnent des résultats différent, SVM_1 j'ai remplacé alpha*Y pas beta et minimisé par rapport à beta, et SVM_2 c'est le normal comme la formule du cours. Je pense que SVM_1 a des résultats plus logiques, faut prendre C=5 environ.\n",
    "\n",
    "Au début j'ai voulu faire la méthode du cours mais rien marchait, je me suis donc inspiré d'un truc sur github où ils utilisaient le changement de variable avec beta donc j'ai appliqué ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "concrete-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Kernel)\n",
    "\n",
    "x_train = X_train0[:1500]\n",
    "y_train = Y_train0[:1500]\n",
    "x_val = X_train0[1500:]\n",
    "y_val = Y_train0[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(estimator):\n",
    "    def __init__(self, Kernel, C = 1):\n",
    "        self.kernel = Kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        \n",
    "    def fit(self, K, Y):\n",
    "        #Je fais le changement de variabe: b= alpha * diag(Y)\n",
    "        #donc je résoud: min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C\n",
    "        n = len(Y)\n",
    "\n",
    "        #take Y as -1 and 1\n",
    "        label = 2 * Y - 1\n",
    "       \n",
    "        #Create the matrix for the solver\n",
    "        P = cvxopt.matrix(np.outer(label, label) * K ) \n",
    "        q = cvxopt.matrix(-np.ones(n))\n",
    "        A = cvxopt.matrix(label, (1,n), 'd')\n",
    "        b = cvxopt.matrix(0.0)\n",
    "                \n",
    "        '''Je réécris l'inégalité : 0 <= b <= C avec C = 1/(2*lambda*n)\n",
    "        comme: G*b <= h avec G = stack(ones(1),-ones(1)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        '''\n",
    "        # b <= C\n",
    "        G1 = np.eye(n)\n",
    "        h1 = np.ones(n) * self.C\n",
    "        \n",
    "        # -b <= 0\n",
    "        G2 = -np.eye(n)\n",
    "        h2 = np.zeros(n)\n",
    "        \n",
    "        G = cvxopt.matrix(np.vstack((G2, G1)))\n",
    "        h = cvxopt.matrix(np.hstack((h2, h1)))\n",
    "\n",
    "        #min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C\n",
    "        solver = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        self.beta = np.ravel(solver['x'])\n",
    "        \n",
    "        #Je retire les vecteurs avec un beta trop petit\n",
    "        eps = 1e-5\n",
    "        supportIndices = np.abs(self.beta) < eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "\n",
    "        self.alpha = self.beta * label\n",
    "        self.alpha[supportIndices] = np.zeros(n)[supportIndices]  #On met les alpha à 0 quand ils sont plus petit que eps\n",
    "        \n",
    "        #Calcul du Bias\n",
    "        for i in range(n):\n",
    "            self.b = label[i]\n",
    "            self.b -= np.sum( self.alpha * K[i, :])\n",
    "        self.b /= n\n",
    "\n",
    "    def set_parameter(self,C): \n",
    "        self.C = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "decreased-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kernel_train = to_Kernel_train(x_train, GaussianKernel)\n",
    "Kernel_test = to_Kernel_test(x_train, x_val , GaussianKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "vital-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM_1(Kernel = GaussianKernel, C=0.004)\n",
    "model.fit(Kernel_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "israeli-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SVM = model.predict(Kernel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "innocent-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1]\n",
      "0.576\n"
     ]
    }
   ],
   "source": [
    "print(prediction_SVM[0:100])\n",
    "print(sum(prediction_SVM == y_val)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "assigned-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(model, Kernel_train, Y_train, parameters): \n",
    "    '''\n",
    "    grid search cv. \n",
    "    put the model you want, for example Estimators.KRR(), the Kernel_train you want to train on, the labels Y_train\n",
    "    and the list of parameters (parameters) you want to try. \n",
    "    \n",
    "    Will print the best parameter with the score associated. \n",
    "    '''\n",
    "    scores = list()\n",
    "    for parameter in parameters : \n",
    "        model.set_parameter(parameter)\n",
    "        scores.append(model.cross_val(Kernel_train,Y_train, 5))\n",
    "        print('We have tested  : ', np.round(parameter, 6))\n",
    "    arg_max = np.argmax(np.array(scores))\n",
    "    print('###########BEST_PARAM = ',np.round(parameters[arg_max], 6), 'with score :', scores[arg_max])\n",
    "    return parameters[arg_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "renewable-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kernel_cv = to_Kernel_train(X_train[0], GaussianKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "intelligent-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:19,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.554\n",
      "We have tested  :  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:19,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5529999999999999\n",
      "We have tested  :  0.085857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:19,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5575\n",
      "We have tested  :  0.071714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:18,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5475\n",
      "We have tested  :  0.057571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:18,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5495\n",
      "We have tested  :  0.043429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:19,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5625\n",
      "We have tested  :  0.029286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:19,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.569\n",
      "We have tested  :  0.015143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:18,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.5185\n",
      "We have tested  :  0.001\n",
      "###########BEST_PARAM =  0.015143 with score : 0.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.015142857142857152"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = np.linspace(0.1,0.001,8)\n",
    "grid_search_cv(model, Kernel_cv, Y_train[0], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "reduced-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_C(X, Y, C_list, Kernel, cv = 4):\n",
    "    acc = []\n",
    "    for c in C_list:\n",
    "        print('____ For C = ', c, '____')\n",
    "        model = SVM_1(Kernel, C=c)\n",
    "        acc.append(model.cross_val(X, Y, cv))\n",
    "    best_param = C_list[np.argmax(acc)]\n",
    "    print('The best value for C is', best_param, 'we get', np.max(acc))\n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ready-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1939 support vectors out of 2000 vectors\n",
      "Model 0 Predicted\n",
      "We keep  1866 support vectors out of 2000 vectors\n",
      "Model 1 Predicted\n",
      "We keep  1696 support vectors out of 2000 vectors\n",
      "Model 2 Predicted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cs = [0.003, 20, 5]\n",
    "download_results(GaussianKernel, cs,'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ordinary-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____ For C =  1 ____\n",
      "We keep  1477 support vectors out of 1500 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:57, 117.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1485 support vectors out of 1500 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:04, 122.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1469 support vectors out of 1500 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [06:26, 131.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1479 support vectors out of 1500 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [08:35, 128.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage of errors :  0.4285\n",
      "The best value for C is 1 we get 0.4285\n",
      "CPU times: user 9min 5s, sys: 32.3 s, total: 9min 38s\n",
      "Wall time: 8min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time best_c = grid_search_C(X_train1, Y_train1, [1], Kernel = Polynomial_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "geological-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We keep  1304 support vectors.\n",
      "[1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0] [0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
      " 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1]\n",
      "accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "model = SVM_1(kernel = GaussianKernel, C=5)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "results = model.predict(x_val)\n",
    "\n",
    "print( results[:100], y_val[:100])\n",
    "n = results == y_val\n",
    "print('accuracy:', sum(n)/len(results))\n",
    "#print(model.alpha, len(model.alpha_), model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-panel",
   "metadata": {},
   "source": [
    "### Autre SVM (pas besoin car SVM_1 fonctionne très bien):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "accompanied-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec cvxopt pour résoudre min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C  \n",
    "#(en gros b= alpha * diag(Y))\n",
    "class estimator(): \n",
    "    def __init__(self , Kernel): \n",
    "        self.Kernel = Kernel\n",
    "        self.Kernel_train = None \n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        \n",
    "    def predict_proba(self,Kernel_test): \n",
    "        if (self.alpha == None).any()==True  : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            return  sigmoid(self.alpha@Kernel_test + self.b)\n",
    "    \n",
    "    def predict(self,K_test): \n",
    "        if (self.alpha == None).any()==True : \n",
    "            print(\"Il faut d'abord fitter les données\")\n",
    "        else : \n",
    "            prob = self.predict_proba(K_test)\n",
    "            return (prob>0)*1\n",
    "        \n",
    "    def cross_val(self, K_train,ytrain,cv): \n",
    "        mistake = 0\n",
    "        Kerneltrainsplit,Kernelvalsplit,ytrainsplit,yvalsplit = Kernel_cross_val_split(K_train,ytrain,cv)\n",
    "        for Ktrain,Kval,ytrain,yval in tqdm(zip(Kerneltrainsplit,Kernelvalsplit, ytrainsplit, yvalsplit)):\n",
    "            self.fit(Ktrain,ytrain)\n",
    "            pred = self.predict(Kval)\n",
    "            mistake+=np.sum(np.abs(pred-yval))\n",
    "        print('Score : ',1- mistake/K_train.shape[0])\n",
    "        return 1- mistake/K_train.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SVM_1(estimator):\n",
    "    def __init__(self, Kernel, C = 1):\n",
    "        self.kernel = Kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        \n",
    "    def fit(self, K, Y):\n",
    "        n = len(Y)\n",
    "        #calculate the kernel\n",
    "        #K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "    \n",
    "        lbd = 1\n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        #take Y as -1 and 1\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        \n",
    "        P = cvxopt.matrix(np.outer(label, label) * K ) \n",
    "        q = cvxopt.matrix(-np.ones(n))\n",
    "        A = cvxopt.matrix(label, (1,n), 'd')\n",
    "        b = cvxopt.matrix(0.0)\n",
    "                \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        ça revient au même et je crois que le solver devrait fonctionner avec ça, mais j'y arrive pas encore\n",
    "        '''\n",
    "        # b <= C\n",
    "        G1 = np.eye(n)\n",
    "        h1 = np.ones(n) * self.C\n",
    "        \n",
    "        # -b <= 0\n",
    "        G2 = -np.eye(n)\n",
    "        h2 = np.zeros(n)\n",
    "        \n",
    "        G = cvxopt.matrix(np.vstack((G2, G1)))\n",
    "        h = cvxopt.matrix(np.hstack((h2, h1)))\n",
    "\n",
    "        #min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C\n",
    "        solver = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        self.beta = np.ravel(solver['x']) # le alpha ou on garde toutes les coordonnées \n",
    "                                                # en comparaison avec le self.alpha ou en garde que quelques-uns \n",
    "        \n",
    "        #Je retire les vecteurs avec un alpha trop petit\n",
    "        eps = 1e-5\n",
    "        supportIndices = np.abs(self.beta) < eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        #self.support_vectors = X[supportIndices]\n",
    "        #self.support_Y = label[supportIndices]\n",
    "        #self.alpha = self.beta[supportIndices]  \n",
    "        #alpha : all_alpha sans les alpha < eps        \n",
    "        #print('We keep ', len(self.alpha), 'support vectors out of',len(self.all_alpha),'vectors')\n",
    "        \n",
    "        #Pour prédire avec classe estimator:\n",
    "        #self.X_train = X\n",
    "        self.alpha = self.beta*label\n",
    "        self.alpha[supportIndices] = np.zeros(n)[supportIndices]\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(n):\n",
    "            self.b = label[i]\n",
    "            self.b -= np.sum( self.alpha * K[i, :])\n",
    "        self.b /= n\n",
    "    \n",
    "    def predict2(self, X):\n",
    "        \n",
    "        y_predict = np.zeros(len(X))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            for alpha, sv, label in zip(self.alpha, self.support_vectors):\n",
    "                y_predict[i] += alpha * self.kernel(sv, X[i]) \n",
    "        \n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "        #return y_predict + self.b\n",
    "        \n",
    "    def set_parameter(self,C): \n",
    "        self.C = C \n",
    "    '''\n",
    "    def cross_val(self, Xtrain, ytrain, cv): \n",
    "        mistake = 0\n",
    "        Xtrainsplit, Xvalsplit, ytrainsplit, yvalsplit = cross_val_split(Xtrain, ytrain, cv)\n",
    "        for xtrain, xval, ytrain, yval in tqdm(zip(Xtrainsplit, Xvalsplit, ytrainsplit, yvalsplit)):\n",
    "            self.fit(xtrain, ytrain)\n",
    "            pred = self.predict(xval)\n",
    "            print('Accuracy :', 1 - np.sum(np.abs(pred - yval)) / pred.shape[0])\n",
    "            mistake += np.sum(np.abs(pred - yval))\n",
    "        print('Average accuracy: ', 1 - mistake / Xtrain.shape[0])\n",
    "        return 1 - mistake / Xtrain.shape[0]\n",
    "    '''\n",
    "    \n",
    "#model = SVM_1(Kernel = GaussianKernel, C=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "extreme-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avec cvxopt pour résoudre min_a 1/2 * alpha.T * K * alpha - alpha.T * Y s.t. 0<=y*alpha<=C\n",
    "class SVM_2:\n",
    "    def __init__(self, kernel = GaussianKernel, C = 1):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        n = len(Y)        \n",
    "        #K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "                \n",
    "        K = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        #lbd = 1\n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        # P=K et q=-Y\n",
    "        P = cvxopt.matrix(K) \n",
    "        q = cvxopt.matrix(-label, tc='d')\n",
    "                \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        '''\n",
    "        \n",
    "        # Condition 0 <= alpha_i * Y_i <= C\n",
    "        G1 = np.diag(-label)\n",
    "        G2 = np.diag(label)\n",
    "        G = cvxopt.matrix(np.vstack((G1, G2)), tc='d')\n",
    "        \n",
    "        h1 = np.zeros((n, 1), dtype='float64')\n",
    "        h2 = self.C * np.ones((n, 1), dtype='float64')\n",
    "        h = cvxopt.matrix(np.vstack((h1, h2)))\n",
    "\n",
    "        # solves min_a 1/2 a^T * P * a + q^T * a s.t. G*a <= h\n",
    "        solver = cvxopt.solvers.qp(P, q, G, h)\n",
    "        \n",
    "        self.alpha = np.ravel(solver['x'])\n",
    "        \n",
    "        #Je retire les vecteurs avec un alpha trop petit\n",
    "        eps = 1e-5\n",
    "        supportIndices = np.abs(self.alpha) > eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        self.support_vectors = X[supportIndices]\n",
    "        self.support_Y = label[supportIndices]\n",
    "        self.alpha_ = self.alpha[supportIndices]  #alpha_ : alpha sans les alpha < eps\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alpha_)):\n",
    "            self.b = self.support_Y[i]\n",
    "            self.b -= np.sum( self.alpha_ * K[ind[i], supportIndices])\n",
    "        self.b /= len(self.alpha_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_predict = np.zeros(len(X))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(self.alpha_)):\n",
    "                y_predict[i] += self.alpha_[j] * self.kernel(self.support_vectors[j], X[i])\n",
    "        \n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "        #return y_predict + self.b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "proud-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM_2(kernel = GaussianKernel, C=25)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "results = model.predict(x_val)\n",
    "\n",
    "print( results[:100], y_val[:100])\n",
    "n = results == y_val\n",
    "print(sum(n))\n",
    "print(model.alpha, len(model.alpha_), model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-concert",
   "metadata": {},
   "source": [
    "EN dessous il y a un autre avec la même méthode que 'SVM_1' mais avec un autre optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "local-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avec qp_solver pour résoudre min_b 1/2 * b.T * diag(Y) * K * diag(Y) * b - b.T * 1 s.t. 0<= b <= C  \n",
    "#(en gros b= alpha * diag(Y))\n",
    "\n",
    "#Il marche et donne le même résultat que cvxopt\n",
    "\n",
    "class SVM_autre:\n",
    "    def __init__(self, kernel=GaussianKernel, C=1):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_Y = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        n = len(Y)        \n",
    "        K = np.apply_along_axis(lambda x1: np.apply_along_axis( lambda x2 : self.kernel(x2, x1), 1, X), 1, X)\n",
    "        \n",
    "        #C = 1 / (2 * n * lbd)  #ça dépend si on veut gérer C ou lambda\n",
    "\n",
    "        label = 2 * Y - 1\n",
    "        \n",
    "        P = np.outer(label, label) * K\n",
    "        q = - np.ones(n)\n",
    "        \n",
    "        '''Je réécris l'inégalité : 0<=y_i*alpha_i<=C avec C = 1/(2*lambda*n)\n",
    "        comme: G*alpha<=h avec G=stack(diag(Y),-diag(Y)) et h= [C, ..., C, 0, ..., 0] (n fois C et n fois 0)\n",
    "        ça revient au même et je crois que le solver devrait fonctionner avec ça, mais j'y arrive pas encore\n",
    "        '''\n",
    "        G = np.vstack((np.eye(n), -np.eye(n)))\n",
    "\n",
    "        h = np.ones(2*n)\n",
    "        h[:n] = h[:n] * self.C\n",
    "        h[n:] = h[n:] * 0\n",
    "\n",
    "        A = label\n",
    "        b = np.array([0.])\n",
    "\n",
    "        self.alpha = solve_qp(P, q.astype('double'), G.astype('double'), h, A, b, solver = 'quadprog')\n",
    "        \n",
    "        #Pour le moment je garde tout X mais faudrait retirer les X dont le alpha est trop bas\n",
    "        eps = 1e-15\n",
    "        supportIndices = self.alpha > eps\n",
    "        ind = np.arange(n)[supportIndices]\n",
    "        \n",
    "        self.support_vectors = X[supportIndices]\n",
    "        self.support_Y = label[supportIndices]\n",
    "        self.alpha_ = self.alpha[supportIndices]\n",
    "        \n",
    "        #Bias\n",
    "        self.b = 0\n",
    "        for i in range(len(self.alpha_)):\n",
    "            self.b = self.support_Y[i]\n",
    "            self.b -= np.sum( self.alpha_ * self.support_Y * K[ind[i], supportIndices])\n",
    "        self.b /= len(self.alpha_)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        y_predict = np.zeros(len(X))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            for alpha, sv, label in zip(self.alpha_, self.support_vectors, self.support_Y):\n",
    "                y_predict[i] += alpha * label * self.kernel(sv, X[i]) \n",
    "\n",
    "        return ((y_predict + self.b) > 0)*1\n",
    "    \n",
    "    def predict_bis(self, X):\n",
    "        \n",
    "        def predict_one(x):\n",
    "            pred = np.apply_along_axis(lambda s: self.kernel(s, x), 1, self.support_vectors)\n",
    "            pred = pred * self.support_Y * self.alpha\n",
    "            return np.sum(pred)\n",
    "        \n",
    "        preds = np.apply_along_axis(predict_one, 1, X)\n",
    "        return 1 * (preds > 0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "supposed-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm = SVM_autre(kernel = GaussianKernel, C=0.01)\n",
    "svm.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "danish-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1] [0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0]\n",
      "[0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00061584\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.00433891 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00264192 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00251599\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.00961544 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00032761 0.0042436  0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.00226446 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.00452394 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.0040861\n",
      " 0.01       0.01       0.00210576 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.00224987\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.00870044 0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.00177014 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01      ]\n"
     ]
    }
   ],
   "source": [
    "results = svm.predict(x_val)\n",
    "print(results[:100], y_val[:100])\n",
    "print(svm.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-latex",
   "metadata": {},
   "source": [
    "Aide svm github : https://github.com/zongmianli/mva-kernel-methods/blob/kernel-challenge/svm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "serial-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02414692  0.00498681  0.01534296  0.03327065  0.02499289  0.01782278\n",
      "  0.00641423  0.01737927  0.00571243  0.03785679  0.02937389 -0.0020743\n",
      "  0.01801311  0.01202195  0.02616614  0.02493651  0.02069579  0.00564455\n",
      " -0.00066408  0.02225113  0.00237909  0.03542867  0.00787691  0.02247312\n",
      "  0.02751154  0.00633758  0.01760011  0.0165434   0.0267509   0.03206684\n",
      "  0.01438477  0.03242849  0.01270201  0.01455076  0.00451208  0.01183673\n",
      "  0.01354231  0.00326173  0.04398204  0.02790272 -0.00445432  0.04707981\n",
      "  0.00802263  0.02043321  0.00878177  0.01557869  0.03094886  0.03406918\n",
      "  0.02125684  0.02878366  0.00324311  0.01825782  0.02390436  0.00269406\n",
      "  0.02602237  0.01234005  0.00085998  0.0158754   0.02489576  0.02026007\n",
      "  0.00903557  0.0151773   0.03625054  0.01268538  0.00022868  0.00617082\n",
      "  0.0134089   0.01244844 -0.00563212  0.00012831  0.02412231  0.02177587\n",
      "  0.01969552  0.03197964  0.01770224  0.01614737  0.01016502  0.03356141\n",
      "  0.00210028  0.02871426  0.00812573 -0.00372503  0.02350311  0.03760823\n",
      "  0.01724742  0.01470091  0.0336364   0.02919009  0.02509313  0.00788259\n",
      "  0.0071019   0.00382853  0.03857615  0.01813762  0.00941578 -0.00600826\n",
      "  0.02025118  0.0257626   0.0119341  -0.00116485] [1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0] [179.53299291473328, 179.2638193446423, 180.06224292004032, 179.98659862549744, 180.16655562663908, 179.22562567027808, 179.75516827499877, 179.66681955881654, 180.18673038466085, 179.70241921540344, 179.75791492680966, 180.00640157214227, 179.74911107109548, 179.44817542047855, 179.7850907331536, 179.5778626028203, 179.5496336538501, 179.8592049366436, 174.77746331094124, 179.59672640457802, 178.35988857531692, 179.8078495329257, 179.31872034329956, 180.07631170977538, 179.578343086164, 179.39241844112905, 180.2470769371597, 179.74626811428834, 179.2629305887961, 179.56186642155092, 180.12031700199594, 179.73210576965843, 179.49982565165803, 179.97592902073393, 179.95039953989735, 179.788659581134, 179.775622431027, 180.20588015176787, 179.3612256550765, 179.7158465553939, 179.2436108196129, 179.31601944226543, 179.26289857068903, 180.05071604001594, 180.0069337865176, 179.98289655308085, 179.79608566334838, 180.04765977314327, 179.84502724551533, 179.88596317123944, 178.68342513554018, 179.87434479060593, 180.02386270412558, 179.6190805627739, 177.7993652170307, 180.01741078269117, 180.094808334175, 180.06057382875395, 179.74661948524238, 179.86970409796655, 179.71353103963804, 180.14250582326866, 179.96612741819962, 180.03915165612278, 179.50714427689144, 180.0398283532452, 179.9683182002998, 179.55131704933473, 179.9211668447428, 179.88145303643572, 179.87976394730816, 179.63064625853465, 179.8714339296601, 179.5202150128781, 178.89877277044604, 179.99463301154293, 179.58198307647763, 179.7409697119273, 179.1261397883086, 179.39255188559272, 179.7519504616132, 179.87882035781223, 180.1244229738326, 179.77478119935347, 180.02566561893596, 179.71273049858195, 179.47839684275363, 179.92173758422285, 179.6568178309584, 180.17560952157513, 179.45673093356072, 179.8877368182291, 179.70823779075891, 180.1933542168511, 179.85672035873722, 179.7460786341896, 179.71641059675997, 179.468887095894, 179.7800981991421, 179.69958266160398]\n"
     ]
    }
   ],
   "source": [
    "#Tests sur les fct de prédiction\n",
    "\n",
    "def predict_one(x):\n",
    "    pred = np.apply_along_axis(lambda s: GaussianKernel(s, x), 1, X_bis)\n",
    "    pred = pred * Y_bis * model.alpha\n",
    "    return np.sum(pred)\n",
    "\n",
    "def f_from_alpha(alpha, Kernel, X):\n",
    "    return  lambda x : np.sum([alpha[i]*Kernel(X[i,:],x) for i in range(X.shape[0])])\n",
    "f_alpha = f_from_alpha(model.alpha, GaussianKernel, x_train)\n",
    "\n",
    "res = [f_alpha(x_val[i]) for i in range(len(y_val))]\n",
    "\n",
    "preds = np.apply_along_axis(predict_one, 1, x_val)\n",
    "#preds = 1 * (preds > 0)\n",
    "print(preds, y_val, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
