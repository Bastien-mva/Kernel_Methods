{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from itertools import product, permutations\n",
    "\n",
    "import numba\n",
    "from numba import njit,vectorize, jit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@njit\n",
    "sig2 = 1\n",
    "@njit\n",
    "def GaussianKernel(x,y):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2/(2*sig2))\n",
    "\n",
    "@njit\n",
    "def PolynomialKernel(x,y): \n",
    "    return (x.dot(y)+c)**d\n",
    "@njit\n",
    "def LinearKernel(x,y): \n",
    "    return x.dot(y)\n",
    "\n",
    "@njit\n",
    "def Laplace_kernel(x, y, gamma=1):\n",
    "    return 0.5 * np.exp(-gamma * np.linalg.norm(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importation_mat_100(): \n",
    "    X_train_100 = []\n",
    "    X_test_100 = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtrain = np.squeeze(xtrain.to_numpy())\n",
    "        X_train_100.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'_mat100.csv',delimiter= ',', header= None)\n",
    "        xtest = np.squeeze(xtest.to_numpy())\n",
    "        X_test_100.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train_100,X_test_100,Y_train\n",
    "\n",
    "def importation(): \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    Y_train = []\n",
    "    for i in range(3): \n",
    "        xtrain = pd.read_csv('data/Xtr'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtrain = xtrain.iloc[1:,1].to_numpy()\n",
    "        X_train.append(xtrain)\n",
    "    \n",
    "        xtest = pd.read_csv('data/Xte'+str(i)+'.csv',delimiter= ',', header= None)\n",
    "        xtest = xtest.iloc[1:,1].to_numpy()\n",
    "        X_test.append(xtest)\n",
    "    \n",
    "        Y_train.append(pd.read_csv('data/Ytr'+str(i)+'.csv',delimiter= ',')['Bound'].to_numpy())\n",
    "    return X_train,X_test,Y_train\n",
    "\n",
    "X_train,X_test,Y_train = importation()\n",
    "X_train_100, X_test_100,Y_train = importation_mat_100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traintest = []\n",
    "for i in range(3): \n",
    "    X_traintest.append(np.concatenate((X_train[i],X_test[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Kernel_train(X, Kernel): \n",
    "    length = X.shape[0]\n",
    "    mat_K = np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        x_i = np.squeeze(X[i])\n",
    "        for j in range(i,length): \n",
    "            x_j = np.squeeze(X[j])\n",
    "            value = Kernel(x_i,x_j)\n",
    "            mat_K[i,j] = value\n",
    "            mat_K[j,i] = value \n",
    "    return mat_K\n",
    "\n",
    "#@njit \n",
    "def to_Kernel_test(Xtrain,Xtest,Kernel):\n",
    "    '''\n",
    "    takes the training data input Xtrain and the test data Xtest and computes the Kernel_test. \n",
    "    \n",
    "    The length of the resulting Kernel_test will be (nb_traing_samples, nb_testing_samples) \n",
    "    '''\n",
    "    length_train = Xtrain.shape[0]\n",
    "    length_test = Xtest.shape[0]\n",
    "    bimat_K = np.zeros((length_train,length_test))\n",
    "    for i in range(length_train):\n",
    "        x_i = np.squeeze(Xtrain[i])\n",
    "        for j in range(length_test): \n",
    "            x_j = np.squeeze(Xtest[j])\n",
    "            bimat_K[i,j] = Kernel(x_i,x_j)\n",
    "    return bimat_K\n",
    "\n",
    "def standardize(K): \n",
    "    '''\n",
    "    standardize the given matrix K\n",
    "    '''\n",
    "    U = np.full(K.shape,1/K.shape[0])\n",
    "    I = np.eye(K.shape[0])\n",
    "    return (I-U)@K@(I-U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute phi for Spectrum kernel\n",
    "def phi_spectrum(x,k,U):\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi_spec = np.zeros(len(U))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in range(len(x)-k+1):\n",
    "            if x[i:i+k] == u:\n",
    "                phi_spec[j] +=1\n",
    "    return phi_spec\n",
    "\n",
    "#Compute phi for Substring kernel\n",
    "def l(i):\n",
    "    return i[-1] - i[0] + 1\n",
    "\n",
    "def I(k,n):\n",
    "    I = set()\n",
    "    for ele in permutations(range(0,n),k):\n",
    "        I.add(tuple(sorted(list(ele))))\n",
    "    return list(I)\n",
    "\n",
    "def phi_substring(x,k,U,lamb=0.5): #fonctionne (testé avec l'exemple page 392)\n",
    "    \"\"\"U: list of the sequences of size k to look for in x\"\"\"\n",
    "    phi = np.zeros(len(U))\n",
    "    I_kn = I(k,len(x))\n",
    "    for j, u in enumerate(U):\n",
    "        for i in I_kn:\n",
    "            x_i = \"\".join([x[idx]  for idx in i])\n",
    "            if x_i==u:\n",
    "                phi[j] += lamb**l(i)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def make_dict_phi(X, phi,k):\n",
    "    U = [''.join(letter) for letter in product('ACGT', repeat=k)]\n",
    "    phi_dict = {seq:phi(seq,k,U) for seq in tqdm(X)}\n",
    "    return phi_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [01:35<00:00, 94.49it/s] \n",
      "100%|██████████| 9000/9000 [06:13<00:00, 24.08it/s]\n"
     ]
    }
   ],
   "source": [
    "##CONSTRUCTION DES DICTIONNAIRES. On fait ça pour eviter de les recalculer à chaque fois\n",
    "dict_spectrum_traintest = [0]*10\n",
    "\n",
    "#dict_spectrum_traintest[3] = make_dict_phi(X_traintest[0],phi_spectrum,3)\n",
    "#dict_spectrum_traintest[4] = make_dict_phi(X_traintest[0],phi_spectrum,4)\n",
    "\n",
    "#dict_spectrum_traintest[5] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,5)\n",
    "#dict_spectrum_traintest[6] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,6)\n",
    "dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)\n",
    "#dict_spectrum_traintest[8] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,8)\n",
    "#dict_sub_traintest_2 = make_dict_phi(X_traintest[0], phi_substring,2)\n",
    "\n",
    "#dict_sub_traintest_3 = make_dict_phi(X_traintest[0], phi_substring,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [24:52<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_spectrum_traintest[7] = make_dict_phi(np.concatenate(X_traintest),phi_spectrum,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_spectrum(k): \n",
    "    '''\n",
    "    template function to build a K_spectrum_function. \n",
    "    We need to have one function for each kernel functionin order to compute efficiently the Kernel matrix \n",
    "    '''\n",
    "    def K_spectrum(x,y): \n",
    "        value = np.sum([dict_spectrum_traintest[k][str(x)] * dict_spectrum_traintest[k][str(y)]])\n",
    "        return value \n",
    "    return K_spectrum\n",
    "K_spectrum_5 = make_K_spectrum(5)\n",
    "K_spectrum_6 = make_K_spectrum(6)\n",
    "K_spectrum_7 = make_K_spectrum(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 2., 3.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_spectrum_traintest[5][X_train[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_Kernels(Kernel): \n",
    "    '''\n",
    "    gets the first Kernel_train and Kernel_test. This is to \n",
    "    Permet d'avoir les Kernels de train et de test, au moins pour tester le modèle, voir ce qu'il donne sur le premier dataset. \n",
    "    '''\n",
    "    K_train = to_Kernel_train(X_train[0], Kernel)\n",
    "    K_test = to_Kernel_test(X_train[0], X_test[0], Kernel)\n",
    "    return K_train,K_test \n",
    "\n",
    "def get_list_Kernels(Kernel) : \n",
    "    '''\n",
    "    Permet d'avoir la liste des Kernel_train et Kernel_test pour chaque deteset. renvoie deux listes : la premiere contient \n",
    "    les 3 premiers Kernel_train des 3 datasets. Le deuxieme renvoie les Kernel_test des 3 datasets. \n",
    "    \n",
    "    Ne marche qu'avec les kernels qui prenent des strings en entree. \n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train[i],X_test[i], Kernel))\n",
    "    return list_K_train,list_K_test\n",
    "\n",
    "def get_list_Kernels(Kernel) : \n",
    "    '''\n",
    "    Permet d'avoir la liste des Kernel_train et Kernel_test pour chaque deteset. renvoie deux listes : la premiere contient \n",
    "    les 3 premiers Kernel_train des 3 datasets. Le deuxieme renvoie les Kernel_test des 3 datasets. \n",
    "    \n",
    "    Ne marche qu'avec les kernels qui prenent des strings en entree. \n",
    "    '''\n",
    "    list_K_train = list()\n",
    "    list_K_test = list()\n",
    "    for i in tqdm(range(3)): \n",
    "        list_K_train.append(to_Kernel_train(X_train[i], Kernel))\n",
    "        list_K_test.append(to_Kernel_test(X_train[i],X_test[i], Kernel))\n",
    "    return list_K_train,list_K_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:33<00:00, 111.33s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_6, list_K_test_spectrum_6 = get_list_Kernels(K_spectrum_6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:21<00:00, 107.27s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_5, list_K_test_spectrum_5 = get_list_Kernels(K_spectrum_5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [09:34<00:00, 191.66s/it]\n"
     ]
    }
   ],
   "source": [
    "list_K_train_spectrum_7, list_K_test_spectrum_7 = get_list_Kernels(K_spectrum_7)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 14.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7025\n",
      "We have tested  :  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.72\n",
      "We have tested  :  0.006993103448275863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.81it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.726\n",
      "We have tested  :  0.013886206896551725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.92it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7195\n",
      "We have tested  :  0.02077931034482759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.99it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7444999999999999\n",
      "We have tested  :  0.02767241379310345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.39it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.731\n",
      "We have tested  :  0.034565517241379316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  8.32it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7384999999999999\n",
      "We have tested  :  0.04145862068965518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.64it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.735\n",
      "We have tested  :  0.04835172413793104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.12it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7395\n",
      "We have tested  :  0.055244827586206906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.53it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.741\n",
      "We have tested  :  0.06213793103448277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.749\n",
      "We have tested  :  0.06903103448275863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.739\n",
      "We have tested  :  0.07592413793103449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.98it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.733\n",
      "We have tested  :  0.08281724137931036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.737\n",
      "We have tested  :  0.08971034482758622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7395\n",
      "We have tested  :  0.09660344827586208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.737\n",
      "We have tested  :  0.10349655172413795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.15it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7295\n",
      "We have tested  :  0.11038965517241381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 10.13it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.733\n",
      "We have tested  :  0.11728275862068967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.32it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7375\n",
      "We have tested  :  0.12417586206896554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.08it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7295\n",
      "We have tested  :  0.13106896551724137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.31it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7375\n",
      "We have tested  :  0.13796206896551724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.43it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7364999999999999\n",
      "We have tested  :  0.14485517241379312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7315\n",
      "We have tested  :  0.15174827586206896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.18it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.731\n",
      "We have tested  :  0.15864137931034483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  9.25it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.729\n",
      "We have tested  :  0.1655344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.05it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.718\n",
      "We have tested  :  0.17242758620689655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.91it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7335\n",
      "We have tested  :  0.17932068965517242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 12.25it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.7255\n",
      "We have tested  :  0.1862137931034483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 11.11it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.735\n",
      "We have tested  :  0.19310689655172414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.743\n",
      "We have tested  :  0.2\n",
      "###########BEST_PARAM =  0.06903103448275863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06903103448275863"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grid_search_cv(model, Kernel_train, Y_train, parameters): \n",
    "    scores = list()\n",
    "    for parameter in parameters : \n",
    "        model.set_parameter(parameter)\n",
    "        scores.append(model.cross_val(Kernel_train,Y_train, 5))\n",
    "        print('We have tested  : ', parameter)\n",
    "    arg_max = np.argmax(np.array(scores))\n",
    "    print('###########BEST_PARAM = ',parameters[arg_max])\n",
    "    return parameters[arg_max]\n",
    "KRR_spectrum_7 = Estimators.KRR(Kernel = K_spectrum_7)\n",
    "parameters = np.linspace(0.0001,0.2,30)\n",
    "#parameters = 10**np.linspace(-10,0,15)\n",
    "grid_search_cv(KRR_spectrum_7, list_K_train_spectrum_7[2], Y_train[2], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_results(model,list_Kernel_train,list_Kernel_test, parameters, name_dossier):\n",
    "    Y_predicted = []\n",
    "    for i in range(3): \n",
    "        model.set_parameter(parameters[i])\n",
    "        model.fit(list_Kernel_train[i], Y_train[i])\n",
    "        Y_predicted.append(model.predict(list_Kernel_test[i])*1)\n",
    "        print('Model {} Predicted'.format(i))\n",
    "\n",
    "    d = { 'Id' : np.arange(3000), 'Bound' : np.concatenate(Y_predicted)}\n",
    "    out = pd.DataFrame(data=d)\n",
    "    out.to_csv('predictions_KM'+name_dossier+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 Predicted\n",
      "Model 1 Predicted\n",
      "Model 2 Predicted\n"
     ]
    }
   ],
   "source": [
    "best_parameters = [0.2057,1.1275,0.06903]\n",
    "download_results(KRR_spectrum_7,list_K_train_spectrum_7,list_K_test_spectrum_7,best_parameters,'K_spectrum_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloads_from_voting(models, lists_Kernel_train,lists_Kernel_test,parameterss, name_dossier): \n",
    "    '''\n",
    "    Takes some models and computes the results by a voting process. \n",
    "    lists_Kernel_train is a list of list_Kernel_train. same for lists_Kernel_test. \n",
    "    parameters is a list of list of parameter (the best parameter possible)\n",
    "    name_dossier is the name of the dossier you want. should be informative\n",
    "    It automatically downloads the predictions\n",
    "    '''\n",
    "    Y_predicted = []\n",
    "    length = len(models)\n",
    "    for i in range(3): \n",
    "        y_pred = list()\n",
    "        for model,list_Kernel_train,list_Kernel_test,parameters in zip(models, lists_Kernel_train, lists_Kernel_test, parameterss): \n",
    "            model.set_parameter(parameters[i])\n",
    "            model.fit(list_Kernel_train[i], Y_train[i])\n",
    "            y_pred.append(model.predict(list_Kernel_test[i]))\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred[:,0][0]= False\n",
    "        y_pred[0] = y_pred[0]*2-0.5\n",
    "        print(y_pred[:,0])\n",
    "        Y_predicted.append(np.sum(y_pred,axis = 1)>0.5)\n",
    "        \n",
    "    ##### rajouter la fin et verifier que ca marche, plus mettre set_parameter dans le SVM de Louis \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True]\n",
      "[ True  True]\n",
      "[ True False]\n"
     ]
    }
   ],
   "source": [
    "models = [KRR_spectrum_7,KRR_spectrum_7]\n",
    "lists_Kernel_train = [list_K_train_spectrum_7, list_K_train_spectrum_7]\n",
    "lists_Kernel_test = [list_K_test_spectrum_7, list_K_test_spectrum_7]\n",
    "parameterss = [[0.2057,1.1275,0.06903], [0.3636,0.21428,0.2057]]\n",
    "name_dossier = 'voting_two_K_spectrum_7'\n",
    "downloads_from_voting(models, lists_Kernel_train, lists_Kernel_test, parameterss,name_dossier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
